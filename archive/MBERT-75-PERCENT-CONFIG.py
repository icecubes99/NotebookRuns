# ============================================================================
# mBERT CONFIGURATION - OPTIMIZED FOR >75% MACRO-F1
# COPY THIS ENTIRE CELL INTO YOUR NOTEBOOK (Replace Cell 8 - Section 3)
# ============================================================================

# üéØ TARGET: STABLE 75-80% F1 ACROSS ALL CLASSES
# ‚è±Ô∏è TRAINING TIME: ~55-70 minutes (worth it for +14% F1 gain!)
# üìä CURRENT: 60.7% ‚Üí TARGET: 75%+ macro-F1

# ===== DATA CONFIGURATION =====
data_path = '/content/adjudications_2025-10-22.csv'
CSV_PATH = '/content/adjudications_2025-10-22.csv'

TITLE_COL = "Title"
TEXT_COL  = "Comment"
SENT_COL  = "Final Sentiment"
POL_COL   = "Final Polarization"

# ===== MODEL CONFIGURATION =====
MODEL_CONFIGS = {
    "mbert": {"name": "bert-base-multilingual-cased", "desc": "mBERT (104 langs)"},
}
MODELS_TO_RUN = ["mbert"]  # ‚Üê TRAINING ONLY mBERT
OUT_DIR = "./runs_mbert_optimized"  # ‚Üê Separate output directory

# ============================================================================
# TIER 1: CRITICAL IMPROVEMENTS (+8-10% F1)
# ============================================================================

# üî• CORE TRAINING - DOUBLED FOR BETTER CONVERGENCE
MAX_LENGTH = 224
EPOCHS = 12                 # üî• DOUBLED (was 6) - Full convergence
BATCH_SIZE = 16            # üî• INCREASED (was 12) - More stable
LR = 2.5e-5               # üî• HIGHER (was 1.5e-5) - Faster learning
WEIGHT_DECAY = 0.03       # üî• STRONGER (was 0.02) - Better regularization
WARMUP_RATIO = 0.20       # üî• LONGER (was 0.10) - Stable start
EARLY_STOP_PATIENCE = 6   # üî• DOUBLED (was 3) - Don't stop early
GRAD_ACCUM_STEPS = 3      # Effective batch: 48 (was 36)

# üî• AGGRESSIVE CLASS WEIGHTS - FIX WEAK CLASSES
CLASS_WEIGHT_MULT = {
    "sentiment": {
        "negative": 1.10,    # Slight reduction (was 1.20)
        "neutral":  1.80,    # üî• MASSIVE BOOST (was 1.00) - Fixes 49% F1!
        "positive": 1.30     # Modest boost (was 1.15)
    },
    "polarization": {
        "non_polarized": 1.20,  # Boost (was 1.00)
        "objective":     2.50,  # üî• HUGE BOOST (was 1.50) - Fixes 40% F1!
        "partisan":      0.95   # Slight reduction (was 1.00)
    }
}
MAX_CLASS_WEIGHT = 10.0  # üî• INCREASED (was 6.0) - Allow stronger weighting

# üî• EXTREME OVERSAMPLING - FOCUS ON RARE CLASSES
USE_OVERSAMPLING = True
USE_JOINT_OVERSAMPLING = True
USE_SMART_OVERSAMPLING = True
JOINT_ALPHA = 0.70              # üî• MUCH MORE AGGRESSIVE (was 0.50)
JOINT_OVERSAMPLING_MAX_MULT = 8.0  # üî• DOUBLED (was 4.0)
OBJECTIVE_BOOST_MULT = 6.0      # üî• TRIPLED (was 2.5) - Massive objective focus
NEUTRAL_BOOST_MULT = 2.5        # üî• NEW - Also boost neutral class

# ============================================================================
# TIER 2: ARCHITECTURE ENHANCEMENTS (+3-5% F1)
# ============================================================================

# üî• LARGER MODEL CAPACITY
HEAD_HIDDEN = 768           # üî• DOUBLED (was 384) - Match BERT size
HEAD_DROPOUT = 0.25         # üî• INCREASED (was 0.20) - More regularization
REP_POOLING = "last4_mean"  # Keep best pooling
HEAD_LAYERS = 3             # üî• INCREASED (was 2) - Deeper heads

# ============================================================================
# TIER 3: ADVANCED TECHNIQUES (+2-4% F1)
# ============================================================================

# üî• MORE AGGRESSIVE LOSS FUNCTIONS
USE_FOCAL_SENTIMENT = True
USE_FOCAL_POLARITY = True
FOCAL_GAMMA_SENTIMENT = 2.0    # üî• DOUBLED (was 1.0) - More focus on hard examples
FOCAL_GAMMA_POLARITY = 2.5     # üî• INCREASED (was 1.5) - Even stronger for polarity
LABEL_SMOOTH_SENTIMENT = 0.10  # üî• INCREASED (was 0.08)
LABEL_SMOOTH_POLARITY = 0.08   # üî• INCREASED (was 0.05)

# üî• TASK WEIGHTS - BALANCE BETTER
TASK_LOSS_WEIGHTS = {
    "sentiment": 1.0,
    "polarization": 1.3  # üî• INCREASED (was 1.2) - More focus on harder task
}

# üî• ENHANCED REGULARIZATION
USE_RDROP = True
RDROP_ALPHA = 0.6           # üî• INCREASED (was 0.4) - Stronger consistency
RDROP_WARMUP_EPOCHS = 2     # üî• LONGER (was 1) - Gradual warmup

# üî• LLRD - LAYER-WISE LEARNING RATE DECAY
USE_LLRD = True
LLRD_DECAY = 0.90           # üî• MORE AGGRESSIVE (was 0.95) - Bigger difference between layers
HEAD_LR_MULT = 3.0          # üî• INCREASED (was 2.0) - Heads learn much faster

# ============================================================================
# TIER 4: TRAINING OPTIMIZATION (+1-2% F1)
# ============================================================================

# üî• STABILITY PARAMETERS
MAX_GRAD_NORM = 0.5              # üî• TIGHTER (was 1.0) - More stable gradients
USE_GRADIENT_CHECKPOINTING = True  # Memory efficiency

# ============================================================================
# MONITORING & OUTPUT
# ============================================================================

import os
os.makedirs(OUT_DIR, exist_ok=True)

# End timing for section 3
timer.end_section("SECTION 3: Configuration Setup")
timer.start_section("SECTION 4: Data Loading & Preprocessing")

print("="*70)
print("üéØ mBERT OPTIMIZATION FOR 75%+ MACRO-F1")
print("="*70)
print(f"üìä Configuration Loaded:")
print(f"   ‚îú‚îÄ Training Epochs: {EPOCHS} (doubled from 6)")
print(f"   ‚îú‚îÄ Effective Batch Size: {BATCH_SIZE * GRAD_ACCUM_STEPS}")
print(f"   ‚îú‚îÄ Learning Rate: {LR}")
print(f"   ‚îú‚îÄ Head Hidden Size: {HEAD_HIDDEN} (doubled from 384)")
print(f"   ‚îú‚îÄ Max Class Weight: {MAX_CLASS_WEIGHT}")
print(f"   ‚îî‚îÄ Output Directory: {OUT_DIR}")
print()
print(f"‚öñÔ∏è  Critical Class Weights:")
print(f"   ‚îú‚îÄ Neutral:   {CLASS_WEIGHT_MULT['sentiment']['neutral']}x (was 1.0x) - Fixes 49% F1")
print(f"   ‚îî‚îÄ Objective: {CLASS_WEIGHT_MULT['polarization']['objective']}x (was 1.5x) - Fixes 40% F1")
print()
print(f"üìà Oversampling Strategy:")
print(f"   ‚îú‚îÄ Max Multiplier: {JOINT_OVERSAMPLING_MAX_MULT}x (was 4.0x)")
print(f"   ‚îú‚îÄ Objective Boost: {OBJECTIVE_BOOST_MULT}x (was 2.5x)")
print(f"   ‚îî‚îÄ Neutral Boost: {NEUTRAL_BOOST_MULT}x (NEW!)")
print()
print(f"‚è±Ô∏è  Expected Training Time: ~55-70 minutes")
print(f"üéØ Target Performance:")
print(f"   ‚îú‚îÄ Sentiment F1:     ‚â•75% (current: 61.7%)")
print(f"   ‚îú‚îÄ Polarization F1:  ‚â•75% (current: 59.6%)")
print(f"   ‚îî‚îÄ Overall Macro-F1: ‚â•75% (current: 60.7%)")
print()
print(f"üî• Key Optimizations:")
print(f"   ‚úÖ TIER 1: Aggressive class weights + oversampling")
print(f"   ‚úÖ TIER 2: Doubled model capacity (768 hidden)")
print(f"   ‚úÖ TIER 3: Stronger focal loss + regularization")
print(f"   ‚úÖ TIER 4: Optimized training schedule")
print()
print(f"üìä Expected Improvement: +14-21% F1")
print(f"   Current:  60.7% macro-F1")
print(f"   Expected: 75-82% macro-F1 ‚úÖ")
print("="*70)

