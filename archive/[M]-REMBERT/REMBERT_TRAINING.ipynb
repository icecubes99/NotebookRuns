{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8507148",
   "metadata": {},
   "source": [
    "## SECTION 1: LOCAL ENVIRONMENT SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c84e145c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU memory management environment variables set\n",
      "   PYTORCH_CUDA_ALLOC_CONF = expandable_segments:True,max_split_size_mb:128\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CRITICAL: Set environment variables BEFORE importing PyTorch\n",
    "# ============================================================================\n",
    "import os\n",
    "\n",
    "# Prevent GPU memory fragmentation - COMBINE both settings\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'\n",
    "\n",
    "print(\"âœ… GPU memory management environment variables set\")\n",
    "print(f\"   PYTORCH_CUDA_ALLOC_CONF = {os.environ.get('PYTORCH_CUDA_ALLOC_CONF')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0074f125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\School\\NotebookRuns\\venv_rembert\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOCAL ENVIRONMENT CHECK ===\n",
      "Python version: 3.13.5\n",
      "PyTorch version: 2.6.0+cu124\n",
      "Transformers version: 4.57.1\n",
      "NumPy version: 2.1.1\n",
      "Pandas version: 2.2.3\n",
      "\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.4\n",
      "GPU Count: 1\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n",
      "GPU Memory: 12.00 GB\n",
      "\n",
      "âœ… Environment check complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: LOCAL ENVIRONMENT SETUP (Windows + RTX 3060)\n",
    "# ============================================================================\n",
    "\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from packaging import version\n",
    "\n",
    "print(\"=== LOCAL ENVIRONMENT CHECK ===\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "print(f\"\\nCUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Verify transformers version\n",
    "assert version.parse(transformers.__version__) >= version.parse(\"4.26.0\"), \\\n",
    "    \"Transformers too old for modern TrainingArguments.\"\n",
    "\n",
    "print(\"\\nâœ… Environment check complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b1c0e",
   "metadata": {},
   "source": [
    "### SECTION 1.5: Compatibility Shim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b680fe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version loaded in memory: 4.57.1\n",
      "Sample of supported TrainingArguments kwargs: ['accelerator_config', 'adafactor', 'adam_beta1', 'adam_beta2', 'adam_epsilon', 'auto_find_batch_size', 'average_tokens_across_devices', 'batch_eval_metrics', 'bf16', 'bf16_full_eval', 'data_seed', 'dataloader_drop_last'] ...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1.5: TRAININGARGUMENTS COMPATIBILITY SHIM\n",
    "# ============================================================================\n",
    "\n",
    "import inspect\n",
    "import transformers as _tf\n",
    "\n",
    "print(\"Transformers version loaded in memory:\", _tf.__version__)\n",
    "\n",
    "def _supported_kwargs_of_training_args():\n",
    "    try:\n",
    "        from transformers import TrainingArguments\n",
    "        sig = inspect.signature(TrainingArguments.__init__)\n",
    "        return set(sig.parameters.keys())\n",
    "    except Exception as e:\n",
    "        print(\"[Compat] Could not inspect TrainingArguments:\", e)\n",
    "        return set()\n",
    "\n",
    "_SUPPORTED_TA_KEYS = _supported_kwargs_of_training_args()\n",
    "print(\"Sample of supported TrainingArguments kwargs:\", sorted(list(_SUPPORTED_TA_KEYS))[:12], \"...\")\n",
    "\n",
    "def make_training_args_compat(**kwargs):\n",
    "    \"\"\"Create TrainingArguments while dropping any unsupported kwargs.\"\"\"\n",
    "    from transformers import TrainingArguments\n",
    "    \n",
    "    # Handle evaluation_strategy -> eval_strategy rename\n",
    "    if \"evaluation_strategy\" in kwargs and \"eval_strategy\" not in kwargs:\n",
    "        kwargs[\"eval_strategy\"] = kwargs.pop(\"evaluation_strategy\")\n",
    "    \n",
    "    filtered = {k: v for k, v in kwargs.items() if k in _SUPPORTED_TA_KEYS}\n",
    "    ignored = [k for k in kwargs.keys() if k not in _SUPPORTED_TA_KEYS]\n",
    "    if ignored:\n",
    "        print(\"[Compat] Ignored unsupported TrainingArguments keys:\", ignored)\n",
    "    return TrainingArguments(**filtered)\n",
    "\n",
    "def get_early_stopping_callbacks(patience: int):\n",
    "    \"\"\"Return EarlyStoppingCallback if available; otherwise return [].\"\"\"\n",
    "    try:\n",
    "        from transformers import EarlyStoppingCallback\n",
    "        return [EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    "    except Exception as e:\n",
    "        print(\"[Compat] EarlyStoppingCallback unavailable:\", e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3480d",
   "metadata": {},
   "source": [
    "## SECTION 2: IMPORTS AND TIMING UTILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8287770f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting SECTION 2: Environment & Imports...\n",
      "âœ… SECTION 2: Environment & Imports completed in 4.0s\n",
      "ðŸ•’ Total runtime so far: 4.0s\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸš€ Starting SECTION 3: Configuration Setup...\n",
      "âœ… SECTION 2: Environment & Imports completed in 4.0s\n",
      "ðŸ•’ Total runtime so far: 4.0s\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸš€ Starting SECTION 3: Configuration Setup...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: IMPORTS AND BASIC SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# ============================================================================\n",
    "# TIMING UTILITY - Track execution time for each section\n",
    "# ============================================================================\n",
    "class SectionTimer:\n",
    "    def __init__(self):\n",
    "        self.section_times = {}\n",
    "        self.start_time = None\n",
    "        self.total_start = time.time()\n",
    "\n",
    "    def start_section(self, section_name):\n",
    "        \"\"\"Start timing a section\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        print(f\"\\nðŸš€ Starting {section_name}...\")\n",
    "\n",
    "    def end_section(self, section_name):\n",
    "        \"\"\"End timing and display results\"\"\"\n",
    "        if self.start_time is None:\n",
    "            self.start_time = time.time()\n",
    "\n",
    "        elapsed = time.time() - self.start_time\n",
    "        self.section_times[section_name] = elapsed\n",
    "\n",
    "        # Format time nicely\n",
    "        if elapsed < 60:\n",
    "            time_str = f\"{elapsed:.1f}s\"\n",
    "        elif elapsed < 3600:\n",
    "            time_str = f\"{elapsed/60:.1f}m {elapsed%60:.0f}s\"\n",
    "        else:\n",
    "            time_str = f\"{elapsed/3600:.1f}h {(elapsed%3600)/60:.0f}m\"\n",
    "\n",
    "        total_elapsed = time.time() - self.total_start\n",
    "        if total_elapsed < 60:\n",
    "            total_str = f\"{total_elapsed:.1f}s\"\n",
    "        elif total_elapsed < 3600:\n",
    "            total_str = f\"{total_elapsed/60:.1f}m {total_elapsed%60:.0f}s\"\n",
    "        else:\n",
    "            total_str = f\"{total_elapsed/3600:.1f}h {(total_elapsed%3600)/60:.0f}m\"\n",
    "\n",
    "        print(f\"âœ… {section_name} completed in {time_str}\")\n",
    "        print(f\"ðŸ•’ Total runtime so far: {total_str}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    def get_summary(self):\n",
    "        \"\"\"Get timing summary\"\"\"\n",
    "        total = time.time() - self.total_start\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"â±ï¸  EXECUTION TIME SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        for section, elapsed in self.section_times.items():\n",
    "            if elapsed < 60:\n",
    "                time_str = f\"{elapsed:.1f}s\"\n",
    "            elif elapsed < 3600:\n",
    "                time_str = f\"{elapsed/60:.1f}m {elapsed%60:.0f}s\"\n",
    "            else:\n",
    "                time_str = f\"{elapsed/3600:.1f}h {(elapsed%3600)/60:.0f}m\"\n",
    "            print(f\"{section:<40} : {time_str}\")\n",
    "\n",
    "        if total < 60:\n",
    "            total_str = f\"{total:.1f}s\"\n",
    "        elif total < 3600:\n",
    "            total_str = f\"{total/60:.1f}m {total%60:.0f}s\"\n",
    "        else:\n",
    "            total_str = f\"{total/3600:.1f}h {(total%3600)/60:.0f}m\"\n",
    "\n",
    "        print(f\"{'='*40} : {'='*10}\")\n",
    "        print(f\"{'TOTAL EXECUTION TIME':<40} : {total_str}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "# Initialize global timer\n",
    "timer = SectionTimer()\n",
    "timer.start_section(\"SECTION 2: Environment & Imports\")\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "timer.end_section(\"SECTION 2: Environment & Imports\")\n",
    "timer.start_section(\"SECTION 3: Configuration Setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05fe274a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, random, json, math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, TrainingArguments, Trainer,\n",
    "    DataCollatorWithPadding, EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "def seed_all(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_all(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1111bf42",
   "metadata": {},
   "source": [
    "## SECTION 3: CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b909c74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ¤– RemBERT LOCAL TRAINING - RUN #1 BASELINE\n",
      "======================================================================\n",
      "ðŸ“Š Hardware:\n",
      "   GPU: RTX 3060 (12GB VRAM)\n",
      "   Python: 3.13.5\n",
      "   PyTorch: 2.6.0+cu124\n",
      "   Transformers: 4.57.1\n",
      "\n",
      "ðŸŽ¯ Model: google/rembert (110 languages, decoupled embeddings)\n",
      "   Expected Performance: 65-70% macro-F1\n",
      "   Expected Time: 50-75 minutes\n",
      "   vs mBERT (63.06%): +2-7% target\n",
      "   vs XLM-R (67.80%): competitive\n",
      "\n",
      "ðŸ“ Training Configuration:\n",
      "   MAX_LENGTH: 224 (balanced)\n",
      "   Epochs: 18\n",
      "   Batch Size: 8 (effective: 40)\n",
      "   Learning Rate: 2.5e-05\n",
      "   Weight Decay: 0.035\n",
      "   Warmup Ratio: 0.22\n",
      "   Early Stop Patience: 7\n",
      "\n",
      "ðŸ—ï¸  Architecture:\n",
      "   Head Hidden: 768\n",
      "   Head Layers: 3\n",
      "   Head Dropout: 0.24\n",
      "   Pooling: last4_mean\n",
      "\n",
      "âš–ï¸  Class Weighting:\n",
      "   Sentiment: {'negative': 1.15, 'neutral': 1.2, 'positive': 1.4}\n",
      "   Polarization: {'non_polarized': 1.2, 'objective': 2.05, 'partisan': 1.05}\n",
      "\n",
      "ðŸ“Š Oversampling:\n",
      "   Joint Alpha: 0.65\n",
      "   Max Multiplier: 4.6x\n",
      "   Objective Boost: 1.75x\n",
      "   Neutral Boost: 0.9x\n",
      "\n",
      "ðŸ”¥ Advanced Techniques:\n",
      "   Focal Loss: Î³_sent=2.5, Î³_pol=2.8\n",
      "   R-Drop: Î±=0.6, warmup=2 epochs\n",
      "   LLRD: decay=0.88, head_mult=3.5x\n",
      "\n",
      "ðŸ’¾ Output: ./runs_rembert\n",
      "======================================================================\n",
      "âœ… SECTION 3: Configuration Setup completed in 0.0s\n",
      "ðŸ•’ Total runtime so far: 4.0s\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸš€ Starting SECTION 4: Data Loading & Preprocessing...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# REMBERT LOCAL TRAINING - RUN #1 BASELINE\n",
    "# Expected: 50-75 min training time, 65-70% macro-F1 target\n",
    "# Hardware: RTX 3060 (12GB VRAM), Python 3.13.5, PyTorch 2.6.0+cu124\n",
    "# ============================================================================\n",
    "\n",
    "# Data paths (LOCAL - Windows)\n",
    "CSV_PATH = 'd:/School/NotebookRuns/augmented_adjudications_2025-10-22.csv'\n",
    "\n",
    "TITLE_COL = \"Title\"\n",
    "TEXT_COL  = \"Comment\"\n",
    "SENT_COL  = \"Final Sentiment\"\n",
    "POL_COL   = \"Final Polarization\"\n",
    "\n",
    "# Model configuration - RemBERT\n",
    "MODEL_CONFIGS = {\n",
    "    \"rembert\": {\"name\": \"google/rembert\", \"desc\": \"RemBERT (110 langs, decoupled embeddings)\"},\n",
    "}\n",
    "MODELS_TO_RUN = [\"rembert\"]\n",
    "OUT_DIR = \"./runs_rembert\"\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING CONFIGURATION - RTX 3060 OPTIMIZED (12GB VRAM)\n",
    "# Strategy: Balance between mBERT (63%) and XLM-R (68%) configurations\n",
    "# REDUCED: Batch size lowered to prevent system memory crash\n",
    "# ============================================================================\n",
    "MAX_LENGTH = 224              # Reduced from 256 (save memory)\n",
    "EPOCHS = 18                   # Same as XLM-R Run #14 (proven optimal)\n",
    "BATCH_SIZE = 8                # REDUCED from 14 to prevent crash\n",
    "LR = 2.5e-5                  # Between mBERT (2.5e-5) and XLM-R (3.0e-5)\n",
    "WEIGHT_DECAY = 0.035         # Average of mBERT (0.03) and XLM-R (0.04)\n",
    "WARMUP_RATIO = 0.22          # Average of mBERT (0.20) and XLM-R (0.25)\n",
    "EARLY_STOP_PATIENCE = 7      # Average of mBERT (8) and XLM-R (6)\n",
    "GRAD_ACCUM_STEPS = 5         # Effective batch: 40 (was 42)\n",
    "\n",
    "# Per-task loss configuration\n",
    "USE_FOCAL_SENTIMENT = True\n",
    "USE_FOCAL_POLARITY  = True\n",
    "FOCAL_GAMMA_SENTIMENT = 2.5\n",
    "FOCAL_GAMMA_POLARITY = 2.8\n",
    "LABEL_SMOOTH_SENTIMENT = 0.10\n",
    "LABEL_SMOOTH_POLARITY = 0.08\n",
    "\n",
    "# Task weights\n",
    "TASK_LOSS_WEIGHTS = {\"sentiment\": 1.0, \"polarization\": 1.4}\n",
    "\n",
    "# Stability parameters\n",
    "MAX_GRAD_NORM = 1.0\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "\n",
    "# Learning rate scheduling\n",
    "LR_SCHEDULER_TYPE = \"cosine\"\n",
    "NUM_CYCLES = 0.5\n",
    "\n",
    "# ============================================================================\n",
    "# CLASS WEIGHTS - BALANCED APPROACH\n",
    "# Using XLM-R Run #14 proven configuration\n",
    "# ============================================================================\n",
    "CLASS_WEIGHT_MULT = {\n",
    "    \"sentiment\": {\n",
    "        \"negative\": 1.15,\n",
    "        \"neutral\":  1.20,\n",
    "        \"positive\": 1.40\n",
    "    },\n",
    "    \"polarization\": {\n",
    "        \"non_polarized\": 1.20,\n",
    "        \"objective\":     2.05,\n",
    "        \"partisan\":      1.05\n",
    "    }\n",
    "}\n",
    "MAX_CLASS_WEIGHT = 8.0\n",
    "\n",
    "# ============================================================================\n",
    "# OVERSAMPLING STRATEGY\n",
    "# Using XLM-R Run #14 proven configuration\n",
    "# ============================================================================\n",
    "USE_OVERSAMPLING = True\n",
    "USE_JOINT_OVERSAMPLING = True\n",
    "USE_SMART_OVERSAMPLING = True\n",
    "JOINT_ALPHA = 0.65\n",
    "JOINT_OVERSAMPLING_MAX_MULT = 4.6\n",
    "OBJECTIVE_BOOST_MULT = 1.75\n",
    "NEUTRAL_BOOST_MULT = 0.90\n",
    "\n",
    "# ============================================================================\n",
    "# ARCHITECTURE - BALANCED CONFIGURATION\n",
    "# Using proven simple architecture (mBERT/XLM-R lessons)\n",
    "# ============================================================================\n",
    "HEAD_HIDDEN = 768\n",
    "HEAD_DROPOUT = 0.24\n",
    "REP_POOLING = \"last4_mean\"\n",
    "HEAD_LAYERS = 3\n",
    "\n",
    "# ============================================================================\n",
    "# REGULARIZATION - PROVEN TECHNIQUES\n",
    "# ============================================================================\n",
    "USE_RDROP = True\n",
    "RDROP_ALPHA = 0.6\n",
    "RDROP_WARMUP_EPOCHS = 2\n",
    "\n",
    "# LLRD (layer-wise learning-rate decay)\n",
    "USE_LLRD = True\n",
    "LLRD_DECAY = 0.88\n",
    "HEAD_LR_MULT = 3.5\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION SUMMARY\n",
    "# ============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ¤– RemBERT LOCAL TRAINING - RUN #1 BASELINE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ðŸ“Š Hardware:\")\n",
    "print(f\"   GPU: RTX 3060 (12GB VRAM)\")\n",
    "print(f\"   Python: {sys.version.split()[0]}\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   Transformers: {transformers.__version__}\")\n",
    "print()\n",
    "print(f\"ðŸŽ¯ Model: google/rembert (110 languages, decoupled embeddings)\")\n",
    "print(f\"   Expected Performance: 65-70% macro-F1\")\n",
    "print(f\"   Expected Time: 50-75 minutes\")\n",
    "print(f\"   vs mBERT (63.06%): +2-7% target\")\n",
    "print(f\"   vs XLM-R (67.80%): competitive\")\n",
    "print()\n",
    "print(f\"ðŸ“ Training Configuration:\")\n",
    "print(f\"   MAX_LENGTH: {MAX_LENGTH} (balanced)\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRAD_ACCUM_STEPS})\")\n",
    "print(f\"   Learning Rate: {LR}\")\n",
    "print(f\"   Weight Decay: {WEIGHT_DECAY}\")\n",
    "print(f\"   Warmup Ratio: {WARMUP_RATIO}\")\n",
    "print(f\"   Early Stop Patience: {EARLY_STOP_PATIENCE}\")\n",
    "print()\n",
    "print(f\"ðŸ—ï¸  Architecture:\")\n",
    "print(f\"   Head Hidden: {HEAD_HIDDEN}\")\n",
    "print(f\"   Head Layers: {HEAD_LAYERS}\")\n",
    "print(f\"   Head Dropout: {HEAD_DROPOUT}\")\n",
    "print(f\"   Pooling: {REP_POOLING}\")\n",
    "print()\n",
    "print(f\"âš–ï¸  Class Weighting:\")\n",
    "print(f\"   Sentiment: {CLASS_WEIGHT_MULT['sentiment']}\")\n",
    "print(f\"   Polarization: {CLASS_WEIGHT_MULT['polarization']}\")\n",
    "print()\n",
    "print(f\"ðŸ“Š Oversampling:\")\n",
    "print(f\"   Joint Alpha: {JOINT_ALPHA}\")\n",
    "print(f\"   Max Multiplier: {JOINT_OVERSAMPLING_MAX_MULT}x\")\n",
    "print(f\"   Objective Boost: {OBJECTIVE_BOOST_MULT}x\")\n",
    "print(f\"   Neutral Boost: {NEUTRAL_BOOST_MULT}x\")\n",
    "print()\n",
    "print(f\"ðŸ”¥ Advanced Techniques:\")\n",
    "print(f\"   Focal Loss: Î³_sent={FOCAL_GAMMA_SENTIMENT}, Î³_pol={FOCAL_GAMMA_POLARITY}\")\n",
    "print(f\"   R-Drop: Î±={RDROP_ALPHA}, warmup={RDROP_WARMUP_EPOCHS} epochs\")\n",
    "print(f\"   LLRD: decay={LLRD_DECAY}, head_mult={HEAD_LR_MULT}x\")\n",
    "print()\n",
    "print(f\"ðŸ’¾ Output: {OUT_DIR}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "timer.end_section(\"SECTION 3: Configuration Setup\")\n",
    "timer.start_section(\"SECTION 4: Data Loading & Preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc03859",
   "metadata": {},
   "source": [
    "## SECTION 4: DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0e7628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment classes: {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
      "Polarization classes: {0: 'non_polarized', 1: 'objective', 2: 'partisan'}\n",
      "Train size: 9144 Val size: 1959 Test size: 1960\n",
      "Final sentiment class weights: {'negative': 0.8481006622314453, 'neutral': 0.9046748280525208, 'positive': 4.40826416015625}\n",
      "Final polarization class weights: {'non_polarized': 1.2815698385238647, 'objective': 6.1743083000183105, 'partisan': 0.606365978717804}\n",
      "âœ… SECTION 4: Data Loading & Preprocessing completed in 0.1s\n",
      "ðŸ•’ Total runtime so far: 4.1s\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸš€ Starting SECTION 5-9: Model Architecture & Training Setup...\n"
     ]
    }
   ],
   "source": [
    "# ===== Section 4 â€” Load & Prepare Data =====\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "required = [TITLE_COL, TEXT_COL, SENT_COL, POL_COL]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns: {missing}. Found: {list(df.columns)}\")\n",
    "\n",
    "df = df.dropna(subset=[TITLE_COL, TEXT_COL, SENT_COL, POL_COL]).reset_index(drop=True)\n",
    "\n",
    "# Encode labels\n",
    "sent_le = LabelEncoder().fit(df[SENT_COL])\n",
    "pol_le  = LabelEncoder().fit(df[POL_COL])\n",
    "\n",
    "df[\"sent_y\"] = sent_le.transform(df[SENT_COL])\n",
    "df[\"pol_y\"]  = pol_le.transform(df[POL_COL])\n",
    "\n",
    "num_sent_classes = len(sent_le.classes_)\n",
    "num_pol_classes  = len(pol_le.classes_)\n",
    "\n",
    "print(\"Sentiment classes:\", dict(enumerate(sent_le.classes_)))\n",
    "print(\"Polarization classes:\", dict(enumerate(pol_le.classes_)))\n",
    "\n",
    "# Splits (stratify by sentiment)\n",
    "X = df[[TITLE_COL, TEXT_COL]].copy()\n",
    "y_sent = df[\"sent_y\"].values\n",
    "y_pol  = df[\"pol_y\"].values\n",
    "\n",
    "X_train, X_tmp, ysent_train, ysent_tmp, ypol_train, ypol_tmp = train_test_split(\n",
    "    X, y_sent, y_pol, test_size=0.3, random_state=42, stratify=y_sent\n",
    ")\n",
    "X_val, X_test, ysent_val, ysent_test, ypol_val, ypol_test = train_test_split(\n",
    "    X_tmp, ysent_tmp, ypol_tmp, test_size=0.5, random_state=42, stratify=ysent_tmp\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train), \"Val size:\", len(X_val), \"Test size:\", len(X_test))\n",
    "\n",
    "# Balanced class weights from TRAIN only\n",
    "def safe_class_weights(y, n_classes):\n",
    "    classes = np.arange(n_classes)\n",
    "    counts = np.bincount(y, minlength=n_classes)\n",
    "    if np.any(counts == 0):\n",
    "        return np.ones(n_classes, dtype=np.float32)\n",
    "    return compute_class_weight(\"balanced\", classes=classes, y=y).astype(np.float32)\n",
    "\n",
    "sent_weights_np = safe_class_weights(ysent_train, num_sent_classes)\n",
    "pol_weights_np  = safe_class_weights(ypol_train,  num_pol_classes)\n",
    "\n",
    "# Apply user multipliers by class name\n",
    "sent_name_to_idx = {name: i for i, name in enumerate(sent_le.classes_)}\n",
    "pol_name_to_idx  = {name: i for i, name in enumerate(pol_le.classes_)}\n",
    "\n",
    "for cname, mult in CLASS_WEIGHT_MULT[\"sentiment\"].items():\n",
    "    if cname in sent_name_to_idx:\n",
    "        sent_weights_np[sent_name_to_idx[cname]] *= float(mult)\n",
    "\n",
    "for cname, mult in CLASS_WEIGHT_MULT[\"polarization\"].items():\n",
    "    if cname in pol_name_to_idx:\n",
    "        pol_weights_np[pol_name_to_idx[cname]] *= float(mult)\n",
    "\n",
    "# Apply class weight caps\n",
    "sent_weights_np = np.clip(sent_weights_np, 0.1, MAX_CLASS_WEIGHT)\n",
    "pol_weights_np = np.clip(pol_weights_np, 0.1, MAX_CLASS_WEIGHT)\n",
    "\n",
    "print(\"Final sentiment class weights:\", {sent_le.classes_[i]: float(w) for i, w in enumerate(sent_weights_np)})\n",
    "print(\"Final polarization class weights:\", {pol_le.classes_[i]: float(w) for i, w in enumerate(pol_weights_np)})\n",
    "\n",
    "# Save label maps\n",
    "with open(os.path.join(OUT_DIR, \"label_map_sentiment.json\"), \"w\") as f:\n",
    "    json.dump({int(k): v for k, v in dict(enumerate(sent_le.classes_)).items()}, f, indent=2)\n",
    "with open(os.path.join(OUT_DIR, \"label_map_polarization.json\"), \"w\") as f:\n",
    "    json.dump({int(k): v for k, v in dict(enumerate(pol_le.classes_)).items()}, f, indent=2)\n",
    "\n",
    "timer.end_section(\"SECTION 4: Data Loading & Preprocessing\")\n",
    "timer.start_section(\"SECTION 5-9: Model Architecture & Training Setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3780bf",
   "metadata": {},
   "source": [
    "## SECTION 5: DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f9f84e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section 5 â€” Dataset & Collator =====\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TaglishDataset(Dataset):\n",
    "    def __init__(self, titles, texts, y_sent, y_pol, tokenizer, max_length=256):\n",
    "        self.titles = list(titles)\n",
    "        self.texts  = list(texts)\n",
    "        self.y_sent = np.array(y_sent)\n",
    "        self.y_pol  = np.array(y_pol)\n",
    "        self.tok = tokenizer\n",
    "        self.max_length = max_length\n",
    "        # RemBERT uses token_type_ids (like mBERT)\n",
    "        self.use_token_type = \"token_type_ids\" in tokenizer.model_input_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tok(\n",
    "            text=str(self.titles[idx]),\n",
    "            text_pair=str(self.texts[idx]),\n",
    "            truncation=\"only_second\",\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=self.use_token_type,\n",
    "        )\n",
    "        item = {\n",
    "            \"input_ids\": enc[\"input_ids\"],\n",
    "            \"attention_mask\": enc[\"attention_mask\"],\n",
    "            \"sentiment_labels\": torch.tensor(self.y_sent[idx], dtype=torch.long),\n",
    "            \"polarization_labels\": torch.tensor(self.y_pol[idx], dtype=torch.long),\n",
    "        }\n",
    "        if self.use_token_type and \"token_type_ids\" in enc:\n",
    "            item[\"token_type_ids\"] = enc[\"token_type_ids\"]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f531f",
   "metadata": {},
   "source": [
    "## SECTION 6: MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc0c20a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section 6 â€” Multi-Task Model =====\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    summed = (token_embeddings * mask).sum(dim=1)\n",
    "    denom = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    return summed / denom\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, base_model_name: str, num_sent: int, num_pol: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(base_model_name)\n",
    "        self.hidden = self.encoder.config.hidden_size\n",
    "\n",
    "        # Enhanced trunk\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden, HEAD_HIDDEN),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(HEAD_HIDDEN),\n",
    "            nn.Dropout(HEAD_DROPOUT),\n",
    "        )\n",
    "\n",
    "        # Multi-layer heads\n",
    "        if HEAD_LAYERS == 2:\n",
    "            self.head_sent = nn.Sequential(\n",
    "                nn.Linear(HEAD_HIDDEN, HEAD_HIDDEN // 2),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(HEAD_HIDDEN // 2),\n",
    "                nn.Dropout(HEAD_DROPOUT * 0.8),\n",
    "                nn.Linear(HEAD_HIDDEN // 2, num_sent)\n",
    "            )\n",
    "            self.head_pol = nn.Sequential(\n",
    "                nn.Linear(HEAD_HIDDEN, HEAD_HIDDEN // 2),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(HEAD_HIDDEN // 2),\n",
    "                nn.Dropout(HEAD_DROPOUT * 0.8),\n",
    "                nn.Linear(HEAD_HIDDEN // 2, num_pol)\n",
    "            )\n",
    "        elif HEAD_LAYERS >= 3:\n",
    "            self.head_sent = nn.Sequential(\n",
    "                nn.Linear(HEAD_HIDDEN, HEAD_HIDDEN // 2),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(HEAD_HIDDEN // 2),\n",
    "                nn.Dropout(HEAD_DROPOUT * 0.8),\n",
    "                nn.Linear(HEAD_HIDDEN // 2, HEAD_HIDDEN // 4),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(HEAD_HIDDEN // 4),\n",
    "                nn.Dropout(HEAD_DROPOUT * 0.7),\n",
    "                nn.Linear(HEAD_HIDDEN // 4, num_sent)\n",
    "            )\n",
    "            self.head_pol = nn.Sequential(\n",
    "                nn.Linear(HEAD_HIDDEN, HEAD_HIDDEN // 2),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(HEAD_HIDDEN // 2),\n",
    "                nn.Dropout(HEAD_DROPOUT * 0.8),\n",
    "                nn.Linear(HEAD_HIDDEN // 2, HEAD_HIDDEN // 4),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(HEAD_HIDDEN // 4),\n",
    "                nn.Dropout(HEAD_DROPOUT * 0.7),\n",
    "                nn.Linear(HEAD_HIDDEN // 4, num_pol)\n",
    "            )\n",
    "        else:\n",
    "            self.head_sent = nn.Linear(HEAD_HIDDEN, num_sent)\n",
    "            self.head_pol  = nn.Linear(HEAD_HIDDEN, num_pol)\n",
    "\n",
    "        # Enable gradient checkpointing\n",
    "        if USE_GRADIENT_CHECKPOINTING:\n",
    "            self.encoder.gradient_checkpointing_enable()\n",
    "\n",
    "    def _pool(self, outputs, attention_mask):\n",
    "        if REP_POOLING == \"pooler\" and hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n",
    "            return outputs.pooler_output\n",
    "        if REP_POOLING == \"cls\":\n",
    "            return outputs.last_hidden_state[:, 0]\n",
    "        # default: last4_mean\n",
    "        hs = outputs.hidden_states\n",
    "        last4 = torch.stack(hs[-4:]).mean(dim=0)\n",
    "        return mean_pooling(last4, attention_mask)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                sentiment_labels=None,\n",
    "                polarization_labels=None):\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids if token_type_ids is not None else None,\n",
    "            output_hidden_states=(REP_POOLING != \"pooler\")\n",
    "        )\n",
    "        pooled = self._pool(outputs, attention_mask)\n",
    "        z = self.trunk(pooled)\n",
    "        return {\"logits\": (self.head_sent(z), self.head_pol(z))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3867f2",
   "metadata": {},
   "source": [
    "## SECTION 7: METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ade790ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section 7 â€” Metrics =====\n",
    "\n",
    "def compute_metrics_multi(eval_pred):\n",
    "    (sent_logits, pol_logits) = eval_pred.predictions\n",
    "    (y_sent, y_pol) = eval_pred.label_ids\n",
    "\n",
    "    ps = np.argmax(sent_logits, axis=1)\n",
    "    pp = np.argmax(pol_logits, axis=1)\n",
    "\n",
    "    sent_report = classification_report(y_sent, ps, output_dict=True, zero_division=0)\n",
    "    pol_report  = classification_report(y_pol,  pp, output_dict=True, zero_division=0)\n",
    "\n",
    "    sent_f1 = sent_report[\"macro avg\"][\"f1-score\"]\n",
    "    pol_f1  = pol_report[\"macro avg\"][\"f1-score\"]\n",
    "    macro_f1_avg = (sent_f1 + pol_f1) / 2.0\n",
    "\n",
    "    return {\n",
    "        \"sent_acc\": sent_report[\"accuracy\"],\n",
    "        \"sent_prec\": sent_report[\"macro avg\"][\"precision\"],\n",
    "        \"sent_rec\": sent_report[\"macro avg\"][\"recall\"],\n",
    "        \"sent_f1\": sent_f1,\n",
    "\n",
    "        \"pol_acc\": pol_report[\"accuracy\"],\n",
    "        \"pol_prec\": pol_report[\"macro avg\"][\"precision\"],\n",
    "        \"pol_rec\": pol_report[\"macro avg\"][\"recall\"],\n",
    "        \"pol_f1\": pol_f1,\n",
    "\n",
    "        \"macro_f1_avg\": macro_f1_avg\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9dee97",
   "metadata": {},
   "source": [
    "## SECTION 8: CUSTOM TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29ba73e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section 8 â€” Custom Trainer (R-Drop + LLRD) =====\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, gamma=2.0, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    def forward(self, logits, target):\n",
    "        logp = F.log_softmax(logits, dim=1)\n",
    "        p = torch.exp(logp)\n",
    "        loss = F.nll_loss((1 - p) ** self.gamma * logp, target, weight=self.weight, reduction=\"none\")\n",
    "        return loss.mean() if self.reduction == \"mean\" else loss.sum()\n",
    "\n",
    "def _sym_kl_with_logits(logits1, logits2):\n",
    "    p = F.log_softmax(logits1, dim=-1);  q = F.log_softmax(logits2, dim=-1)\n",
    "    p_exp, q_exp = p.exp(), q.exp()\n",
    "    return 0.5 * (F.kl_div(p, q_exp, reduction=\"batchmean\") + F.kl_div(q, p_exp, reduction=\"batchmean\"))\n",
    "\n",
    "class MultiTaskTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, task_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights or {}\n",
    "        self.task_weights  = task_weights or {\"sentiment\": 1.0, \"polarization\": 1.0}\n",
    "        self._custom_train_sampler = None\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        if self.optimizer is not None:\n",
    "            return self.optimizer\n",
    "        if not USE_LLRD:\n",
    "            self.optimizer = AdamW(self.get_decay_parameter_groups(self.model), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "            return self.optimizer\n",
    "\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
    "        encoder = self.model.encoder\n",
    "        n_layers = getattr(encoder.config, \"num_hidden_layers\", 12)\n",
    "        layers = getattr(getattr(encoder, \"encoder\", encoder), \"layer\", None)\n",
    "        if layers is None:\n",
    "            self.optimizer = AdamW(self.get_decay_parameter_groups(self.model), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "            return self.optimizer\n",
    "\n",
    "        param_groups = []\n",
    "\n",
    "        # Embeddings\n",
    "        emb = getattr(encoder, \"embeddings\", None)\n",
    "        if emb is not None:\n",
    "            lr_emb = LR * (LLRD_DECAY ** n_layers)\n",
    "            decay, nodecay = [], []\n",
    "            for n, p in emb.named_parameters():\n",
    "                (nodecay if any(nd in n for nd in no_decay) else decay).append(p)\n",
    "            if decay:   param_groups.append({\"params\": decay,   \"lr\": lr_emb, \"weight_decay\": WEIGHT_DECAY})\n",
    "            if nodecay: param_groups.append({\"params\": nodecay, \"lr\": lr_emb, \"weight_decay\": 0.0})\n",
    "\n",
    "        # Encoder blocks\n",
    "        for i in range(n_layers):\n",
    "            block = layers[i]\n",
    "            lr_i = LR * (LLRD_DECAY ** (n_layers - 1 - i))\n",
    "            decay, nodecay = [], []\n",
    "            for n, p in block.named_parameters():\n",
    "                (nodecay if any(nd in n for nd in no_decay) else decay).append(p)\n",
    "            if decay:   param_groups.append({\"params\": decay,   \"lr\": lr_i, \"weight_decay\": WEIGHT_DECAY})\n",
    "            if nodecay: param_groups.append({\"params\": nodecay, \"lr\": lr_i, \"weight_decay\": 0.0})\n",
    "\n",
    "        # Pooler\n",
    "        pooler = getattr(encoder, \"pooler\", None)\n",
    "        if pooler is not None:\n",
    "            decay, nodecay = [], []\n",
    "            for n, p in pooler.named_parameters():\n",
    "                (nodecay if any(nd in n for nd in no_decay) else decay).append(p)\n",
    "            if decay:   param_groups.append({\"params\": decay,   \"lr\": LR, \"weight_decay\": WEIGHT_DECAY})\n",
    "            if nodecay: param_groups.append({\"params\": nodecay, \"lr\": LR, \"weight_decay\": 0.0})\n",
    "\n",
    "        # Heads/trunk (highest LR)\n",
    "        head_lr = LR * HEAD_LR_MULT\n",
    "        head_modules = [self.model.trunk, self.model.head_sent, self.model.head_pol]\n",
    "        decay, nodecay = [], []\n",
    "        for m in head_modules:\n",
    "            for n, p in m.named_parameters():\n",
    "                (nodecay if any(nd in n for nd in no_decay) else decay).append(p)\n",
    "        if decay:   param_groups.append({\"params\": decay,   \"lr\": head_lr, \"weight_decay\": WEIGHT_DECAY})\n",
    "        if nodecay: param_groups.append({\"params\": nodecay, \"lr\": head_lr, \"weight_decay\": 0.0})\n",
    "\n",
    "        self.optimizer = AdamW(param_groups, lr=LR)\n",
    "        return self.optimizer\n",
    "\n",
    "    def set_train_sampler(self, sampler):\n",
    "        self._custom_train_sampler = sampler\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        if self.train_dataset is None:\n",
    "            return None\n",
    "        if self._custom_train_sampler is not None:\n",
    "            return DataLoader(\n",
    "                self.train_dataset,\n",
    "                batch_size=self.args.train_batch_size,\n",
    "                sampler=self._custom_train_sampler,\n",
    "                collate_fn=self.data_collator,\n",
    "                drop_last=self.args.dataloader_drop_last,\n",
    "                num_workers=self.args.dataloader_num_workers,\n",
    "                pin_memory=self.args.dataloader_pin_memory,\n",
    "            )\n",
    "        return super().get_train_dataloader()\n",
    "\n",
    "    def _sent_loss_fn(self, weight, logits, target):\n",
    "        if USE_FOCAL_SENTIMENT:\n",
    "            return FocalLoss(weight=weight, gamma=FOCAL_GAMMA_SENTIMENT)(logits, target)\n",
    "        return nn.CrossEntropyLoss(weight=weight, label_smoothing=float(LABEL_SMOOTH_SENTIMENT))(logits, target)\n",
    "\n",
    "    def _pol_loss_fn(self, weight, logits, target):\n",
    "        if USE_FOCAL_POLARITY:\n",
    "            return FocalLoss(weight=weight, gamma=FOCAL_GAMMA_POLARITY)(logits, target)\n",
    "        return nn.CrossEntropyLoss(weight=weight, label_smoothing=float(LABEL_SMOOTH_POLARITY))(logits, target)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        Compute loss with compatibility for transformers 4.57+\n",
    "        Args:\n",
    "            num_items_in_batch: Added in transformers 4.57+ for gradient accumulation\n",
    "        \"\"\"\n",
    "        y_sent = inputs.pop(\"sentiment_labels\")\n",
    "        y_pol  = inputs.pop(\"polarization_labels\")\n",
    "\n",
    "        current_epoch = getattr(self.state, 'epoch', 0) if hasattr(self, 'state') else 0\n",
    "        use_rdrop_now = USE_RDROP and model.training and current_epoch >= RDROP_WARMUP_EPOCHS\n",
    "\n",
    "        if use_rdrop_now:\n",
    "            outputs1 = model(**inputs)\n",
    "            outputs2 = model(**inputs)\n",
    "            s1, p1 = outputs1[\"logits\"]\n",
    "            s2, p2 = outputs2[\"logits\"]\n",
    "\n",
    "            ws = self.class_weights.get(\"sentiment\", None); ws = ws.to(s1.device) if ws is not None else None\n",
    "            wp = self.class_weights.get(\"polarization\", None); wp = wp.to(p1.device) if wp is not None else None\n",
    "\n",
    "            ce_s = 0.5 * (self._sent_loss_fn(ws, s1, y_sent) + self._sent_loss_fn(ws, s2, y_sent))\n",
    "            ce_p = 0.5 * (self._pol_loss_fn(wp,  p1, y_pol)  + self._pol_loss_fn(wp,  p2, y_pol))\n",
    "            kl_s = _sym_kl_with_logits(s1, s2)\n",
    "            kl_p = _sym_kl_with_logits(p1, p2)\n",
    "\n",
    "            w_s = float(self.task_weights.get(\"sentiment\", 1.0))\n",
    "            w_p = float(self.task_weights.get(\"polarization\", 1.0))\n",
    "\n",
    "            rdrop_factor = min(1.0, (current_epoch - RDROP_WARMUP_EPOCHS + 1) / 2.0)\n",
    "            loss = w_s * ce_s + w_p * ce_p + (RDROP_ALPHA * rdrop_factor) * (kl_s + kl_p)\n",
    "            if return_outputs:\n",
    "                return loss, {\"logits\": (s1, p1)}\n",
    "            return loss\n",
    "\n",
    "        # Standard single forward\n",
    "        outputs = model(**inputs)\n",
    "        s, p = outputs[\"logits\"]\n",
    "\n",
    "        ws = self.class_weights.get(\"sentiment\", None); ws = ws.to(s.device) if ws is not None else None\n",
    "        wp = self.class_weights.get(\"polarization\", None); wp = wp.to(p.device) if wp is not None else None\n",
    "\n",
    "        loss_s = self._sent_loss_fn(ws, s, y_sent)\n",
    "        loss_p = self._pol_loss_fn(wp, p, y_pol)\n",
    "\n",
    "        w_s = float(self.task_weights.get(\"sentiment\", 1.0))\n",
    "        w_p = float(self.task_weights.get(\"polarization\", 1.0))\n",
    "        loss = w_s * loss_s + w_p * loss_p\n",
    "\n",
    "        if return_outputs:\n",
    "            outputs = dict(outputs); outputs[\"labels\"] = (y_sent, y_pol)\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        y_sent = inputs.get(\"sentiment_labels\", None)\n",
    "        y_pol  = inputs.get(\"polarization_labels\", None)\n",
    "\n",
    "        model_inputs = {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]}\n",
    "        if \"token_type_ids\" in inputs:\n",
    "            model_inputs[\"token_type_ids\"] = inputs[\"token_type_ids\"]\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**model_inputs)\n",
    "            s, p = outputs[\"logits\"]\n",
    "\n",
    "        loss = None\n",
    "        logits = (s.detach(), p.detach())\n",
    "        labels = (y_sent, y_pol) if isinstance(y_sent, torch.Tensor) and isinstance(y_pol, torch.Tensor) else None\n",
    "        return (loss, logits, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e4e22",
   "metadata": {},
   "source": [
    "## SECTION 9: TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9f72d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SECTION 5-9: Model Architecture & Training Setup completed in 0.1s\n",
      "ðŸ•’ Total runtime so far: 4.2s\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ===== Section 9 â€” Train/Evaluate One Model =====\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "def train_eval_one_model(model_key: str,\n",
    "                         X_tr: pd.DataFrame, X_v: pd.DataFrame, X_te: pd.DataFrame,\n",
    "                         ysent_tr: np.ndarray, ysent_v: np.ndarray, ysent_te: np.ndarray,\n",
    "                         ypol_tr: np.ndarray,  ypol_v: np.ndarray,  ypol_te: np.ndarray,\n",
    "                         sent_w_np: np.ndarray, pol_w_np: np.ndarray):\n",
    "    base_name = MODEL_CONFIGS[model_key][\"name\"]\n",
    "    run_dir = os.path.join(OUT_DIR, f\"{model_key}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_name)\n",
    "    collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "    tr_titles, tr_texts = X_tr[TITLE_COL].values, X_tr[TEXT_COL].values\n",
    "    v_titles,  v_texts  = X_v[TITLE_COL].values, X_v[TEXT_COL].values\n",
    "    te_titles, te_texts = X_te[TITLE_COL].values, X_te[TEXT_COL].values\n",
    "\n",
    "    train_ds = TaglishDataset(tr_titles, tr_texts, ysent_tr, ypol_tr, tokenizer, max_length=MAX_LENGTH)\n",
    "    val_ds   = TaglishDataset(v_titles,  v_texts,  ysent_v,  ypol_v,  tokenizer, max_length=MAX_LENGTH)\n",
    "    test_ds  = TaglishDataset(te_titles, te_texts, ysent_te, ypol_te, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "    model = MultiTaskModel(base_name, num_sent_classes, num_pol_classes).to(device)\n",
    "\n",
    "    sent_w = torch.tensor(sent_w_np, dtype=torch.float32)\n",
    "    pol_w  = torch.tensor(pol_w_np,  dtype=torch.float32)\n",
    "\n",
    "    args = make_training_args_compat(\n",
    "        output_dir=run_dir,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "        lr_scheduler_kwargs={\"num_cycles\": NUM_CYCLES},\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1_avg\",\n",
    "        greater_is_better=True,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_dir=os.path.join(run_dir, \"logs\"),\n",
    "        logging_steps=25,\n",
    "        logging_first_step=True,\n",
    "        save_steps=500,\n",
    "        eval_steps=None,\n",
    "        report_to=\"none\",\n",
    "        seed=42,\n",
    "        remove_unused_columns=False,\n",
    "        eval_accumulation_steps=1,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=0,  # Windows compatibility\n",
    "        max_grad_norm=MAX_GRAD_NORM,\n",
    "        label_smoothing_factor=0.0,\n",
    "        save_total_limit=3,\n",
    "        prediction_loss_only=False\n",
    "    )\n",
    "\n",
    "    callbacks = get_early_stopping_callbacks(EARLY_STOP_PATIENCE)\n",
    "\n",
    "    trainer = MultiTaskTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_multi,\n",
    "        callbacks=callbacks,\n",
    "        class_weights={\"sentiment\": sent_w, \"polarization\": pol_w},\n",
    "        task_weights=TASK_LOSS_WEIGHTS\n",
    "    )\n",
    "\n",
    "    # Enhanced joint oversampling with objective + neutral boost\n",
    "    if USE_OVERSAMPLING and USE_JOINT_OVERSAMPLING:\n",
    "        pair_counts = Counter(zip(ysent_tr.tolist(), ypol_tr.tolist()))\n",
    "        counts = np.array(list(pair_counts.values()), dtype=np.float32)\n",
    "        med = float(np.median(counts)) if len(counts) else 1.0\n",
    "\n",
    "        obj_idx = np.where(pol_le.classes_ == \"objective\")[0][0] if \"objective\" in pol_le.classes_ else 1\n",
    "        neutral_idx = np.where(sent_le.classes_ == \"neutral\")[0][0] if \"neutral\" in sent_le.classes_ else 1\n",
    "\n",
    "        def inv_mult(c):\n",
    "            if c <= 0: return JOINT_OVERSAMPLING_MAX_MULT\n",
    "            return float(np.clip(med / float(c), 1.0, JOINT_OVERSAMPLING_MAX_MULT))\n",
    "\n",
    "        inv_by_pair = {k: inv_mult(v) for k, v in pair_counts.items()}\n",
    "        sample_weights = []\n",
    "\n",
    "        for ys, yp in zip(ysent_tr, ypol_tr):\n",
    "            inv = inv_by_pair.get((int(ys), int(yp)), 1.0)\n",
    "            w = (1.0 - JOINT_ALPHA) * 1.0 + JOINT_ALPHA * inv\n",
    "\n",
    "            if USE_SMART_OVERSAMPLING and int(yp) == obj_idx:\n",
    "                w *= OBJECTIVE_BOOST_MULT\n",
    "            if USE_SMART_OVERSAMPLING and int(ys) == neutral_idx:\n",
    "                w *= NEUTRAL_BOOST_MULT\n",
    "\n",
    "            sample_weights.append(w)\n",
    "\n",
    "        obj_boost_count = sum(1 for i, yp in enumerate(ypol_tr) if int(yp) == obj_idx and sample_weights[i] > 2.0)\n",
    "        neutral_boost_count = sum(1 for i, ys in enumerate(ysent_tr) if int(ys) == neutral_idx and sample_weights[i] > 2.0)\n",
    "        print(f\"ðŸ”¥ Enhanced Oversampling: min={min(sample_weights):.2f}, max={max(sample_weights):.2f}\")\n",
    "        print(f\"   â”œâ”€ Objective boosted samples: {obj_boost_count}\")\n",
    "        print(f\"   â””â”€ Neutral boosted samples: {neutral_boost_count}\")\n",
    "        trainer.set_train_sampler(WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True))\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Test\n",
    "    test_out = trainer.predict(test_ds)\n",
    "    metrics = {f\"test_{k}\": float(v) for k, v in test_out.metrics.items()}\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # Ensure weights exist\n",
    "    model_path = os.path.join(run_dir, \"pytorch_model.bin\")\n",
    "    if not os.path.exists(model_path):\n",
    "        torch.save(trainer.model.state_dict(), model_path)\n",
    "    tokenizer.save_pretrained(run_dir)\n",
    "    \n",
    "    with open(os.path.join(run_dir, \"metrics_test.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    sent_logits, pol_logits = test_out.predictions\n",
    "    ysent_pred = np.argmax(sent_logits, axis=1)\n",
    "    ypol_pred  = np.argmax(pol_logits,  axis=1)\n",
    "\n",
    "    cm_sent = confusion_matrix(ysent_te, ysent_pred, labels=list(range(num_sent_classes)))\n",
    "    cm_pol  = confusion_matrix(ypol_te,  ypol_pred,  labels=list(range(num_pol_classes)))\n",
    "    np.save(os.path.join(run_dir, \"cm_sent.npy\"), cm_sent)\n",
    "    np.save(os.path.join(run_dir, \"cm_pol.npy\"),  cm_pol)\n",
    "\n",
    "    def plot_cm(cm, labels, title, path_png):\n",
    "        fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "        im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "        ax.set_title(title); ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "        ax.set_xticks(range(len(labels))); ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "        ax.set_yticks(range(len(labels))); ax.set_yticklabels(labels)\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "        fig.colorbar(im, ax=ax, fraction=0.046); plt.tight_layout(); plt.savefig(path_png, dpi=160); plt.close(fig)\n",
    "\n",
    "    plot_cm(cm_sent, sent_le.classes_, \"Sentiment Confusion\", os.path.join(run_dir, \"cm_sent.png\"))\n",
    "    plot_cm(cm_pol,  pol_le.classes_,  \"Polarization Confusion\", os.path.join(run_dir, \"cm_pol.png\"))\n",
    "\n",
    "    rep_sent = classification_report(ysent_te, ysent_pred, target_names=sent_le.classes_, digits=4, zero_division=0)\n",
    "    rep_pol  = classification_report(ypol_te,  ypol_pred,  target_names=pol_le.classes_,  digits=4, zero_division=0)\n",
    "    with open(os.path.join(run_dir, \"report_sentiment.txt\"), \"w\") as f: f.write(rep_sent)\n",
    "    with open(os.path.join(run_dir, \"report_polarization.txt\"), \"w\") as f: f.write(rep_pol)\n",
    "\n",
    "    return {\"model_key\": model_key, \"base_name\": base_name, **metrics}, (ysent_pred, ypol_pred)\n",
    "\n",
    "timer.end_section(\"SECTION 5-9: Model Architecture & Training Setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfefab",
   "metadata": {},
   "source": [
    "## SECTION 10: RUN TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c4093f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ GPU Memory Management:\n",
      "   Total: 12.00 GB\n",
      "   Allocated: 0.00 GB\n",
      "   Reserved: 0.00 GB\n",
      "   Free: 12.00 GB\n",
      "âœ… Memory cleared and ready for training\n"
     ]
    }
   ],
   "source": [
    "# ===== GPU Memory Management =====\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# DO NOT SET environment variable here - it was already set in Cell 1!\n",
    "# Environment variable must be set BEFORE PyTorch is imported.\n",
    "\n",
    "# Clear any existing GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Verify GPU status\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ðŸ”§ GPU Memory Management:\")\n",
    "    print(f\"   Total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"   Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"   Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"   Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1024**3:.2f} GB\")\n",
    "    print(f\"âœ… Memory cleared and ready for training\")\n",
    "else:\n",
    "    print(\"âš ï¸ CUDA not available!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b5822d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting SECTION 10: Model Training Execution...\n",
      "\n",
      "=== Running rembert -> google/rembert ===\n",
      "ðŸ”¥ Enhanced Oversampling: min=0.90, max=5.84\n",
      "   â”œâ”€ Objective boosted samples: 168\n",
      "   â””â”€ Neutral boosted samples: 0\n",
      "ðŸ”¥ Enhanced Oversampling: min=0.90, max=5.84\n",
      "   â”œâ”€ Objective boosted samples: 168\n",
      "   â””â”€ Neutral boosted samples: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='4122' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/4122 00:02 < 2:50:22, 0.40 it/s, Epoch 0.01/18]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 2.01 GiB is free. Of the allocated memory 7.64 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m MODELS_TO_RUN:\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Running \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_CONFIGS[key][\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     row, preds = \u001b[43mtrain_eval_one_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mysent_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mysent_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mysent_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mypol_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mypol_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mypol_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43msent_weights_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpol_weights_np\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     results.append(row)\n\u001b[32m     18\u001b[39m     pred_cache[key] = preds\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mtrain_eval_one_model\u001b[39m\u001b[34m(model_key, X_tr, X_v, X_te, ysent_tr, ysent_v, ysent_te, ypol_tr, ypol_v, ypol_te, sent_w_np, pol_w_np)\u001b[39m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   â””â”€ Neutral boosted samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mneutral_boost_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m     trainer.set_train_sampler(WeightedRandomSampler(sample_weights, num_samples=\u001b[38;5;28mlen\u001b[39m(sample_weights), replacement=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[32m    115\u001b[39m test_out = trainer.predict(test_ds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\School\\NotebookRuns\\venv_rembert\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\School\\NotebookRuns\\venv_rembert\\Lib\\site-packages\\transformers\\trainer.py:2740\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2737\u001b[39m     context = implicit_replication\n\u001b[32m   2739\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2740\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2742\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_optimizer_step(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2744\u001b[39m \u001b[38;5;66;03m# get leaning rate before update\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\School\\NotebookRuns\\venv_rembert\\Lib\\site-packages\\accelerate\\optimizer.py:166\u001b[39m, in \u001b[36mAcceleratedOptimizer.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    164\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.step = \u001b[38;5;28mself\u001b[39m._optimizer_patched_step_method\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28mself\u001b[39m.scaler.update()\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accelerate_step_called:\n\u001b[32m    170\u001b[39m         \u001b[38;5;66;03m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\School\\NotebookRuns\\venv_rembert\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:457\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    451\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    454\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    455\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\School\\NotebookRuns\\venv_rembert\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:352\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    350\u001b[39m retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v.item() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m     retval = \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\School\\NotebookRuns\\venv_rembert\\Lib\\site-packages\\accelerate\\optimizer.py:211\u001b[39m, in \u001b[36mpatch_optimizer_step.<locals>.patched_step\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpatched_step\u001b[39m(*args, **kwargs):\n\u001b[32m    210\u001b[39m     accelerated_optimizer._accelerate_step_called = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\School\\NotebookRuns\\venv_rembert\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:140\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m opt = opt_ref()\n\u001b[32m    139\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\School\\NotebookRuns\\venv_rembert\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\School\\NotebookRuns\\venv_rembert\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\School\\NotebookRuns\\venv_rembert\\Lib\\site-packages\\torch\\optim\\adamw.py:232\u001b[39m, in \u001b[36mAdamW.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    229\u001b[39m     amsgrad: \u001b[38;5;28mbool\u001b[39m = group[\u001b[33m\"\u001b[39m\u001b[33mamsgrad\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    230\u001b[39m     beta1, beta2 = cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     has_complex = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m     adamw(\n\u001b[32m    244\u001b[39m         params_with_grad,\n\u001b[32m    245\u001b[39m         grads,\n\u001b[32m   (...)\u001b[39m\u001b[32m    263\u001b[39m         has_complex=has_complex,\n\u001b[32m    264\u001b[39m     )\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\School\\NotebookRuns\\venv_rembert\\Lib\\site-packages\\torch\\optim\\adamw.py:175\u001b[39m, in \u001b[36mAdamW._init_group\u001b[39m\u001b[34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[39m\n\u001b[32m    171\u001b[39m state[\u001b[33m\"\u001b[39m\u001b[33mexp_avg\u001b[39m\u001b[33m\"\u001b[39m] = torch.zeros_like(\n\u001b[32m    172\u001b[39m     p, memory_format=torch.preserve_format\n\u001b[32m    173\u001b[39m )\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m state[\u001b[33m\"\u001b[39m\u001b[33mexp_avg_sq\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreserve_format\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[32m    180\u001b[39m     state[\u001b[33m\"\u001b[39m\u001b[33mmax_exp_avg_sq\u001b[39m\u001b[33m\"\u001b[39m] = torch.zeros_like(\n\u001b[32m    181\u001b[39m         p, memory_format=torch.preserve_format\n\u001b[32m    182\u001b[39m     )\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 2.01 GiB is free. Of the allocated memory 7.64 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# ===== Section 10 â€” Run Training =====\n",
    "\n",
    "timer.start_section(\"SECTION 10: Model Training Execution\")\n",
    "\n",
    "results = []\n",
    "pred_cache = {}\n",
    "\n",
    "for key in MODELS_TO_RUN:\n",
    "    print(f\"\\n=== Running {key} -> {MODEL_CONFIGS[key]['name']} ===\")\n",
    "    row, preds = train_eval_one_model(\n",
    "        key,\n",
    "        X_train, X_val, X_test,\n",
    "        ysent_train, ysent_val, ysent_test,\n",
    "        ypol_train,  ypol_val,  ypol_test,\n",
    "        sent_weights_np, pol_weights_np\n",
    "    )\n",
    "    results.append(row)\n",
    "    pred_cache[key] = preds\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(OUT_DIR, \"summary_results.csv\"), index=False)\n",
    "\n",
    "timer.end_section(\"SECTION 10: Model Training Execution\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ‰ TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a4ca7",
   "metadata": {},
   "source": [
    "## SECTION 11: DETAILED ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfaec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section 11 â€” Detailed Breakdown Reports =====\n",
    "\n",
    "DETAILS_DIR = os.path.join(OUT_DIR, \"details\")\n",
    "os.makedirs(DETAILS_DIR, exist_ok=True)\n",
    "\n",
    "def per_class_breakdown(y_true, y_pred, class_names):\n",
    "    rep = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=list(class_names),\n",
    "        output_dict=True, zero_division=0\n",
    "    )\n",
    "    rows = []\n",
    "    for cname in class_names:\n",
    "        if cname in rep:\n",
    "            rows.append({\n",
    "                \"class\": cname,\n",
    "                \"precision\": rep[cname][\"precision\"],\n",
    "                \"recall\":    rep[cname][\"recall\"],\n",
    "                \"f1\":        rep[cname][\"f1-score\"],\n",
    "                \"support\":   int(rep[cname][\"support\"]),\n",
    "            })\n",
    "        else:\n",
    "            rows.append({\"class\": cname, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"support\": 0})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "all_breakdowns = {}\n",
    "\n",
    "for key in MODELS_TO_RUN:\n",
    "    print(f\"\\n=== Detailed breakdowns for {key} ===\")\n",
    "    ysent_pred, ypol_pred = pred_cache[key]\n",
    "\n",
    "    sent_per_class = per_class_breakdown(ysent_test, ysent_pred, sent_le.classes_)\n",
    "    pol_per_class  = per_class_breakdown(ypol_test,  ypol_pred,  pol_le.classes_)\n",
    "\n",
    "    sent_csv = os.path.join(DETAILS_DIR, f\"{key}_sentiment_per_class.csv\")\n",
    "    pol_csv  = os.path.join(DETAILS_DIR, f\"{key}_polarization_per_class.csv\")\n",
    "    sent_per_class.to_csv(sent_csv, index=False)\n",
    "    pol_per_class.to_csv(pol_csv, index=False)\n",
    "\n",
    "    print(\"\\nSentiment â€” per class:\")\n",
    "    display(sent_per_class)\n",
    "\n",
    "    print(\"\\nPolarization â€” per class:\")\n",
    "    display(pol_per_class)\n",
    "\n",
    "    all_breakdowns[key] = {\n",
    "        \"sentiment_per_class_csv\": sent_csv,\n",
    "        \"polarization_per_class_csv\": pol_csv,\n",
    "    }\n",
    "\n",
    "with open(os.path.join(DETAILS_DIR, \"index.json\"), \"w\") as f:\n",
    "    json.dump(all_breakdowns, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved detailed breakdowns to:\", DETAILS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb61cf",
   "metadata": {},
   "source": [
    "## SECTION 12: FINAL SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a565d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section 12 â€” Final Summary =====\n",
    "\n",
    "timer.get_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š REMBERT RUN #1 - FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: google/rembert\")\n",
    "print(f\"Hardware: RTX 3060 (12GB VRAM)\")\n",
    "print(f\"Dataset: {len(df)} samples (augmented)\")\n",
    "print(f\"Train/Val/Test: {len(X_train)}/{len(X_val)}/{len(X_test)}\")\n",
    "print()\n",
    "print(\"Results:\")\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"  Overall Macro-F1: {row.get('test_macro_f1_avg', 0)*100:.2f}%\")\n",
    "    print(f\"  Sentiment F1: {row.get('test_sent_f1', 0)*100:.2f}%\")\n",
    "    print(f\"  Polarization F1: {row.get('test_pol_f1', 0)*100:.2f}%\")\n",
    "print()\n",
    "print(f\"Output directory: {OUT_DIR}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rembert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
