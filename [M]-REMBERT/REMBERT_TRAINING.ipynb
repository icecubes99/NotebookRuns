{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8507148",
   "metadata": {},
   "source": [
    "## SECTION 1: LOCAL ENVIRONMENT SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074f125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: LOCAL ENVIRONMENT SETUP (Windows + RTX 3060)\n",
    "# ============================================================================\n",
    "\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from packaging import version\n",
    "\n",
    "print(\"=== LOCAL ENVIRONMENT CHECK ===\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "print(f\"\\nCUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Verify transformers version\n",
    "assert version.parse(transformers.__version__) >= version.parse(\"4.26.0\"), \\\n",
    "    \"Transformers too old for modern TrainingArguments.\"\n",
    "\n",
    "print(\"\\nâœ… Environment check complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b1c0e",
   "metadata": {},
   "source": [
    "### SECTION 1.5: Compatibility Shim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b680fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1.5: TRAININGARGUMENTS COMPATIBILITY SHIM\n",
    "# ============================================================================\n",
    "\n",
    "import inspect\n",
    "import transformers as _tf\n",
    "\n",
    "print(\"Transformers version loaded in memory:\", _tf.__version__)\n",
    "\n",
    "def _supported_kwargs_of_training_args():\n",
    "    try:\n",
    "        from transformers import TrainingArguments\n",
    "        sig = inspect.signature(TrainingArguments.__init__)\n",
    "        return set(sig.parameters.keys())\n",
    "    except Exception as e:\n",
    "        print(\"[Compat] Could not inspect TrainingArguments:\", e)\n",
    "        return set()\n",
    "\n",
    "_SUPPORTED_TA_KEYS = _supported_kwargs_of_training_args()\n",
    "print(\"Sample of supported TrainingArguments kwargs:\", sorted(list(_SUPPORTED_TA_KEYS))[:12], \"...\")\n",
    "\n",
    "def make_training_args_compat(**kwargs):\n",
    "    \"\"\"Create TrainingArguments while dropping any unsupported kwargs.\"\"\"\n",
    "    from transformers import TrainingArguments\n",
    "    filtered = {k: v for k, v in kwargs.items() if k in _SUPPORTED_TA_KEYS}\n",
    "    ignored = [k for k in kwargs.keys() if k not in _SUPPORTED_TA_KEYS]\n",
    "    if ignored:\n",
    "        print(\"[Compat] Ignored unsupported TrainingArguments keys:\", ignored)\n",
    "    return TrainingArguments(**filtered)\n",
    "\n",
    "def get_early_stopping_callbacks(patience: int):\n",
    "    \"\"\"Return EarlyStoppingCallback if available; otherwise return [].\"\"\"\n",
    "    try:\n",
    "        from transformers import EarlyStoppingCallback\n",
    "        return [EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    "    except Exception as e:\n",
    "        print(\"[Compat] EarlyStoppingCallback unavailable:\", e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3480d",
   "metadata": {},
   "source": [
    "## SECTION 2: IMPORTS AND TIMING UTILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: IMPORTS AND BASIC SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# ============================================================================\n",
    "# TIMING UTILITY - Track execution time for each section\n",
    "# ============================================================================\n",
    "class SectionTimer:\n",
    "    def __init__(self):\n",
    "        self.section_times = {}\n",
    "        self.start_time = None\n",
    "        self.total_start = time.time()\n",
    "\n",
    "    def start_section(self, section_name):\n",
    "        \"\"\"Start timing a section\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        print(f\"\\nðŸš€ Starting {section_name}...\")\n",
    "\n",
    "    def end_section(self, section_name):\n",
    "        \"\"\"End timing and display results\"\"\"\n",
    "        if self.start_time is None:\n",
    "            self.start_time = time.time()\n",
    "\n",
    "        elapsed = time.time() - self.start_time\n",
    "        self.section_times[section_name] = elapsed\n",
    "\n",
    "        # Format time nicely\n",
    "        if elapsed < 60:\n",
    "            time_str = f\"{elapsed:.1f}s\"\n",
    "        elif elapsed < 3600:\n",
    "            time_str = f\"{elapsed/60:.1f}m {elapsed%60:.0f}s\"\n",
    "        else:\n",
    "            time_str = f\"{elapsed/3600:.1f}h {(elapsed%3600)/60:.0f}m\"\n",
    "\n",
    "        total_elapsed = time.time() - self.total_start\n",
    "        if total_elapsed < 60:\n",
    "            total_str = f\"{total_elapsed:.1f}s\"\n",
    "        elif total_elapsed < 3600:\n",
    "            total_str = f\"{total_elapsed/60:.1f}m {total_elapsed%60:.0f}s\"\n",
    "        else:\n",
    "            total_str = f\"{total_elapsed/3600:.1f}h {(total_elapsed%3600)/60:.0f}m\"\n",
    "\n",
    "        print(f\"âœ… {section_name} completed in {time_str}\")\n",
    "        print(f\"ðŸ•’ Total runtime so far: {total_str}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    def get_summary(self):\n",
    "        \"\"\"Get timing summary\"\"\"\n",
    "        total = time.time() - self.total_start\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"â±ï¸  EXECUTION TIME SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        for section, elapsed in self.section_times.items():\n",
    "            if elapsed < 60:\n",
    "                time_str = f\"{elapsed:.1f}s\"\n",
    "            elif elapsed < 3600:\n",
    "                time_str = f\"{elapsed/60:.1f}m {elapsed%60:.0f}s\"\n",
    "            else:\n",
    "                time_str = f\"{elapsed/3600:.1f}h {(elapsed%3600)/60:.0f}m\"\n",
    "            print(f\"{section:<40} : {time_str}\")\n",
    "\n",
    "        if total < 60:\n",
    "            total_str = f\"{total:.1f}s\"\n",
    "        elif total < 3600:\n",
    "            total_str = f\"{total/60:.1f}m {total%60:.0f}s\"\n",
    "        else:\n",
    "            total_str = f\"{total/3600:.1f}h {(total%3600)/60:.0f}m\"\n",
    "\n",
    "        print(f\"{'='*40} : {'='*10}\")\n",
    "        print(f\"{'TOTAL EXECUTION TIME':<40} : {total_str}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "# Initialize global timer\n",
    "timer = SectionTimer()\n",
    "timer.start_section(\"SECTION 2: Environment & Imports\")\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "timer.end_section(\"SECTION 2: Environment & Imports\")\n",
    "timer.start_section(\"SECTION 3: Configuration Setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fe274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, json, math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, TrainingArguments, Trainer,\n",
    "    DataCollatorWithPadding, EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "def seed_all(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_all(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1111bf42",
   "metadata": {},
   "source": [
    "## SECTION 3: CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b909c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# REMBERT LOCAL TRAINING - RUN #1 BASELINE\n",
    "# Expected: 50-75 min training time, 65-70% macro-F1 target\n",
    "# Hardware: RTX 3060 (12GB VRAM), Python 3.13.5, PyTorch 2.6.0+cu124\n",
    "# ============================================================================\n",
    "\n",
    "# Data paths (LOCAL - Windows)\n",
    "CSV_PATH = 'd:/School/NotebookRuns/augmented_adjudications_2025-10-22.csv'\n",
    "\n",
    "TITLE_COL = \"Title\"\n",
    "TEXT_COL  = \"Comment\"\n",
    "SENT_COL  = \"Final Sentiment\"\n",
    "POL_COL   = \"Final Polarization\"\n",
    "\n",
    "# Model configuration - RemBERT\n",
    "MODEL_CONFIGS = {\n",
    "    \"rembert\": {\"name\": \"google/rembert\", \"desc\": \"RemBERT (110 langs, decoupled embeddings)\"},\n",
    "}\n",
    "MODELS_TO_RUN = [\"rembert\"]\n",
    "OUT_DIR = \"./runs_rembert\"\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING CONFIGURATION - RTX 3060 OPTIMIZED (12GB VRAM)\n",
    "# Strategy: Balance between mBERT (63%) and XLM-R (68%) configurations\n",
    "# ============================================================================\n",
    "MAX_LENGTH = 256              # Balanced (mBERT: 320, XLM-R: 224)\n",
    "EPOCHS = 18                   # Same as XLM-R Run #14 (proven optimal)\n",
    "BATCH_SIZE = 14               # Conservative for 12GB VRAM\n",
    "LR = 2.5e-5                  # Between mBERT (2.5e-5) and XLM-R (3.0e-5)\n",
    "WEIGHT_DECAY = 0.035         # Average of mBERT (0.03) and XLM-R (0.04)\n",
    "WARMUP_RATIO = 0.22          # Average of mBERT (0.20) and XLM-R (0.25)\n",
    "EARLY_STOP_PATIENCE = 7      # Average of mBERT (8) and XLM-R (6)\n",
    "GRAD_ACCUM_STEPS = 3         # Effective batch: 42\n",
    "\n",
    "# Per-task loss configuration\n",
    "USE_FOCAL_SENTIMENT = True\n",
    "USE_FOCAL_POLARITY  = True\n",
    "FOCAL_GAMMA_SENTIMENT = 2.5\n",
    "FOCAL_GAMMA_POLARITY = 2.8\n",
    "LABEL_SMOOTH_SENTIMENT = 0.10\n",
    "LABEL_SMOOTH_POLARITY = 0.08\n",
    "\n",
    "# Task weights\n",
    "TASK_LOSS_WEIGHTS = {\"sentiment\": 1.0, \"polarization\": 1.4}\n",
    "\n",
    "# Stability parameters\n",
    "MAX_GRAD_NORM = 1.0\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "\n",
    "# Learning rate scheduling\n",
    "LR_SCHEDULER_TYPE = \"cosine\"\n",
    "NUM_CYCLES = 0.5\n",
    "\n",
    "# ============================================================================\n",
    "# CLASS WEIGHTS - BALANCED APPROACH\n",
    "# Using XLM-R Run #14 proven configuration\n",
    "# ============================================================================\n",
    "CLASS_WEIGHT_MULT = {\n",
    "    \"sentiment\": {\n",
    "        \"negative\": 1.15,\n",
    "        \"neutral\":  1.20,\n",
    "        \"positive\": 1.40\n",
    "    },\n",
    "    \"polarization\": {\n",
    "        \"non_polarized\": 1.20,\n",
    "        \"objective\":     2.05,\n",
    "        \"partisan\":      1.05\n",
    "    }\n",
    "}\n",
    "MAX_CLASS_WEIGHT = 8.0\n",
    "\n",
    "# ============================================================================\n",
    "# OVERSAMPLING STRATEGY\n",
    "# Using XLM-R Run #14 proven configuration\n",
    "# ============================================================================\n",
    "USE_OVERSAMPLING = True\n",
    "USE_JOINT_OVERSAMPLING = True\n",
    "USE_SMART_OVERSAMPLING = True\n",
    "JOINT_ALPHA = 0.65\n",
    "JOINT_OVERSAMPLING_MAX_MULT = 4.6\n",
    "OBJECTIVE_BOOST_MULT = 1.75\n",
    "NEUTRAL_BOOST_MULT = 0.90\n",
    "\n",
    "# ============================================================================\n",
    "# ARCHITECTURE - BALANCED CONFIGURATION\n",
    "# Using proven simple architecture (mBERT/XLM-R lessons)\n",
    "# ============================================================================\n",
    "HEAD_HIDDEN = 768\n",
    "HEAD_DROPOUT = 0.24\n",
    "REP_POOLING = \"last4_mean\"\n",
    "HEAD_LAYERS = 3\n",
    "\n",
    "# ============================================================================\n",
    "# REGULARIZATION - PROVEN TECHNIQUES\n",
    "# ============================================================================\n",
    "USE_RDROP = True\n",
    "RDROP_ALPHA = 0.6\n",
    "RDROP_WARMUP_EPOCHS = 2\n",
    "\n",
    "# LLRD (layer-wise learning-rate decay)\n",
    "USE_LLRD = True\n",
    "LLRD_DECAY = 0.88\n",
    "HEAD_LR_MULT = 3.5\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION SUMMARY\n",
    "# ============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ¤– RemBERT LOCAL TRAINING - RUN #1 BASELINE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ðŸ“Š Hardware:\")\n",
    "print(f\"   GPU: RTX 3060 (12GB VRAM)\")\n",
    "print(f\"   Python: {sys.version.split()[0]}\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   Transformers: {transformers.__version__}\")\n",
    "print()\n",
    "print(f\"ðŸŽ¯ Model: google/rembert (110 languages, decoupled embeddings)\")\n",
    "print(f\"   Expected Performance: 65-70% macro-F1\")\n",
    "print(f\"   Expected Time: 50-75 minutes\")\n",
    "print(f\"   vs mBERT (63.06%): +2-7% target\")\n",
    "print(f\"   vs XLM-R (67.80%): competitive\")\n",
    "print()\n",
    "print(f\"ðŸ“ Training Configuration:\")\n",
    "print(f\"   MAX_LENGTH: {MAX_LENGTH} (balanced)\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRAD_ACCUM_STEPS})\")\n",
    "print(f\"   Learning Rate: {LR}\")\n",
    "print(f\"   Weight Decay: {WEIGHT_DECAY}\")\n",
    "print(f\"   Warmup Ratio: {WARMUP_RATIO}\")\n",
    "print(f\"   Early Stop Patience: {EARLY_STOP_PATIENCE}\")\n",
    "print()\n",
    "print(f\"ðŸ—ï¸  Architecture:\")\n",
    "print(f\"   Head Hidden: {HEAD_HIDDEN}\")\n",
    "print(f\"   Head Layers: {HEAD_LAYERS}\")\n",
    "print(f\"   Head Dropout: {HEAD_DROPOUT}\")\n",
    "print(f\"   Pooling: {REP_POOLING}\")\n",
    "print()\n",
    "print(f\"âš–ï¸  Class Weighting:\")\n",
    "print(f\"   Sentiment: {CLASS_WEIGHT_MULT['sentiment']}\")\n",
    "print(f\"   Polarization: {CLASS_WEIGHT_MULT['polarization']}\")\n",
    "print()\n",
    "print(f\"ðŸ“Š Oversampling:\")\n",
    "print(f\"   Joint Alpha: {JOINT_ALPHA}\")\n",
    "print(f\"   Max Multiplier: {JOINT_OVERSAMPLING_MAX_MULT}x\")\n",
    "print(f\"   Objective Boost: {OBJECTIVE_BOOST_MULT}x\")\n",
    "print(f\"   Neutral Boost: {NEUTRAL_BOOST_MULT}x\")\n",
    "print()\n",
    "print(f\"ðŸ”¥ Advanced Techniques:\")\n",
    "print(f\"   Focal Loss: Î³_sent={FOCAL_GAMMA_SENTIMENT}, Î³_pol={FOCAL_GAMMA_POLARITY}\")\n",
    "print(f\"   R-Drop: Î±={RDROP_ALPHA}, warmup={RDROP_WARMUP_EPOCHS} epochs\")\n",
    "print(f\"   LLRD: decay={LLRD_DECAY}, head_mult={HEAD_LR_MULT}x\")\n",
    "print()\n",
    "print(f\"ðŸ’¾ Output: {OUT_DIR}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "timer.end_section(\"SECTION 3: Configuration Setup\")\n",
    "timer.start_section(\"SECTION 4: Data Loading & Preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc03859",
   "metadata": {},
   "source": [
    "## SECTION 4: DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section 4 â€” Load & Prepare Data =====\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "required = [TITLE_COL, TEXT_COL, SENT_COL, POL_COL]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns: {missing}. Found: {list(df.columns)}\")\n",
    "\n",
    "df = df.dropna(subset=[TITLE_COL, TEXT_COL, SENT_COL, POL_COL]).reset_index(drop=True)\n",
    "\n",
    "# Encode labels\n",
    "sent_le = LabelEncoder().fit(df[SENT_COL])\n",
    "pol_le  = LabelEncoder().fit(df[POL_COL])\n",
    "\n",
    "df[\"sent_y\"] = sent_le.transform(df[SENT_COL])\n",
    "df[\"pol_y\"]  = pol_le.transform(df[POL_COL])\n",
    "\n",
    "num_sent_classes = len(sent_le.classes_)\n",
    "num_pol_classes  = len(pol_le.classes_)\n",
    "\n",
    "print(\"Sentiment classes:\", dict(enumerate(sent_le.classes_)))\n",
    "print(\"Polarization classes:\", dict(enumerate(pol_le.classes_)))\n",
    "\n",
    "# Splits (stratify by sentiment)\n",
    "X = df[[TITLE_COL, TEXT_COL]].copy()\n",
    "y_sent = df[\"sent_y\"].values\n",
    "y_pol  = df[\"pol_y\"].values\n",
    "\n",
    "X_train, X_tmp, ysent_train, ysent_tmp, ypol_train, ypol_tmp = train_test_split(\n",
    "    X, y_sent, y_pol, test_size=0.3, random_state=42, stratify=y_sent\n",
    ")\n",
    "X_val, X_test, ysent_val, ysent_test, ypol_val, ypol_test = train_test_split(\n",
    "    X_tmp, ysent_tmp, ypol_tmp, test_size=0.5, random_state=42, stratify=ysent_tmp\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train), \"Val size:\", len(X_val), \"Test size:\", len(X_test))\n",
    "\n",
    "# Balanced class weights from TRAIN only\n",
    "def safe_class_weights(y, n_classes):\n",
    "    classes = np.arange(n_classes)\n",
    "    counts = np.bincount(y, minlength=n_classes)\n",
    "    if np.any(counts == 0):\n",
    "        return np.ones(n_classes, dtype=np.float32)\n",
    "    return compute_class_weight(\"balanced\", classes=classes, y=y).astype(np.float32)\n",
    "\n",
    "sent_weights_np = safe_class_weights(ysent_train, num_sent_classes)\n",
    "pol_weights_np  = safe_class_weights(ypol_train,  num_pol_classes)\n",
    "\n",
    "# Apply user multipliers by class name\n",
    "sent_name_to_idx = {name: i for i, name in enumerate(sent_le.classes_)}\n",
    "pol_name_to_idx  = {name: i for i, name in enumerate(pol_le.classes_)}\n",
    "\n",
    "for cname, mult in CLASS_WEIGHT_MULT[\"sentiment\"].items():\n",
    "    if cname in sent_name_to_idx:\n",
    "        sent_weights_np[sent_name_to_idx[cname]] *= float(mult)\n",
    "\n",
    "for cname, mult in CLASS_WEIGHT_MULT[\"polarization\"].items():\n",
    "    if cname in pol_name_to_idx:\n",
    "        pol_weights_np[pol_name_to_idx[cname]] *= float(mult)\n",
    "\n",
    "# Apply class weight caps\n",
    "sent_weights_np = np.clip(sent_weights_np, 0.1, MAX_CLASS_WEIGHT)\n",
    "pol_weights_np = np.clip(pol_weights_np, 0.1, MAX_CLASS_WEIGHT)\n",
    "\n",
    "print(\"Final sentiment class weights:\", {sent_le.classes_[i]: float(w) for i, w in enumerate(sent_weights_np)})\n",
    "print(\"Final polarization class weights:\", {pol_le.classes_[i]: float(w) for i, w in enumerate(pol_weights_np)})\n",
    "\n",
    "# Save label maps\n",
    "with open(os.path.join(OUT_DIR, \"label_map_sentiment.json\"), \"w\") as f:\n",
    "    json.dump({int(k): v for k, v in dict(enumerate(sent_le.classes_)).items()}, f, indent=2)\n",
    "with open(os.path.join(OUT_DIR, \"label_map_polarization.json\"), \"w\") as f:\n",
    "    json.dump({int(k): v for k, v in dict(enumerate(pol_le.classes_)).items()}, f, indent=2)\n",
    "\n",
    "timer.end_section(\"SECTION 4: Data Loading & Preprocessing\")\n",
    "timer.start_section(\"SECTION 5-9: Model Architecture & Training Setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3780bf",
   "metadata": {},
   "source": [
    "## SECTION 5: DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f84e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section 5 â€” Dataset & Collator =====\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TaglishDataset(Dataset):\n",
    "    def __init__(self, titles, texts, y_sent, y_pol, tokenizer, max_length=256):\n",
    "        self.titles = list(titles)\n",
    "        self.texts  = list(texts)\n",
    "        self.y_sent = np.array(y_sent)\n",
    "        self.y_pol  = np.array(y_pol)\n",
    "        self.tok = tokenizer\n",
    "        self.max_length = max_length\n",
    "        # RemBERT uses token_type_ids (like mBERT)\n",
    "        self.use_token_type = \"token_type_ids\" in tokenizer.model_input_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tok(\n",
    "            text=str(self.titles[idx]),\n",
    "            text_pair=str(self.texts[idx]),\n",
    "            truncation=\"only_second\",\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=self.use_token_type,\n",
    "        )\n",
    "        item = {\n",
    "            \"input_ids\": enc[\"input_ids\"],\n",
    "            \"attention_mask\": enc[\"attention_mask\"],\n",
    "            \"sentiment_labels\": torch.tensor(self.y_sent[idx], dtype=torch.long),\n",
    "            \"polarization_labels\": torch.tensor(self.y_pol[idx], dtype=torch.long),\n",
    "        }\n",
    "        if self.use_token_type and \"token_type_ids\" in enc:\n",
    "            item[\"token_type_ids\"] = enc[\"token_type_ids\"]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f531f",
   "metadata": {},
   "source": [
    "## SECTION 6: MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c20a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section 6 â€” Multi-Task Model =====\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    summed = (token_embeddings * mask).sum(dim=1)\n",
    "    denom = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    return summed / denom\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, base_model_name: str, num_sent: int, num_pol: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(base_model_name)\n",
    "        self.hidden = self.encoder.config.hidden_size\n",
    "\n",
    "        # Enhanced trunk\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden, HEAD_HIDDEN),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(HEAD_HIDDEN),\n",
    "            nn.Dropout(HEAD_DROPOUT),\n",
    "        )\n",
    "\n",
    "        # Multi-layer heads\n",
    "        if HEAD_LAYERS == 2:\n",
    "            self.head_sent = nn.Sequential(\n",
    "                nn.Linear(HEAD_HIDDEN, HEAD_HIDDEN // 2),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(HEAD_HIDDEN // 2),\n",
    "                nn.Dropout(HEAD_DROPOUT * 0.8),\n",
    "                nn.Linear(HEAD_HIDDEN // 2, num_sent)\n",
    "            )\n",
    "            self.head_pol = nn.Sequential(\n",
    "                nn.Linear(HEAD_HIDDEN, HEAD_HIDDEN // 2),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(HEAD_HIDDEN // 2),\n",
    "                nn.Dropout(HEAD_DROPOUT * 0.8),\n",
    "                nn.Linear(HEAD_HIDDEN // 2, num_pol)\n",
    "            )\n",
    "        elif HEAD_LAYERS >= 3:\n",
    "            self.head_sent = nn.Sequential(\n",
    "                nn.Linear(HEAD_HIDDEN, HEAD_HIDDEN // 2),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(HEAD_HIDDEN // 2),\n",
    "                nn.Dropout(HEAD_DROPOUT * 0.8),\n",
    "                nn.Linear(HEAD_HIDDEN // 2, HEAD_HIDDEN // 4),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(HEAD_HIDDEN // 4),\n",
    "                nn.Dropout(HEAD_DROPOUT * 0.7),\n",
    "                nn.Linear(HEAD_HIDDEN // 4, num_sent)\n",
    "            )\n",
    "            self.head_pol = nn.Sequential(\n",
    "                nn.Linear(HEAD_HIDDEN, HEAD_HIDDEN // 2),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(HEAD_HIDDEN // 2),\n",
    "                nn.Dropout(HEAD_DROPOUT * 0.8),\n",
    "                nn.Linear(HEAD_HIDDEN // 2, HEAD_HIDDEN // 4),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(HEAD_HIDDEN // 4),\n",
    "                nn.Dropout(HEAD_DROPOUT * 0.7),\n",
    "                nn.Linear(HEAD_HIDDEN // 4, num_pol)\n",
    "            )\n",
    "        else:\n",
    "            self.head_sent = nn.Linear(HEAD_HIDDEN, num_sent)\n",
    "            self.head_pol  = nn.Linear(HEAD_HIDDEN, num_pol)\n",
    "\n",
    "        # Enable gradient checkpointing\n",
    "        if USE_GRADIENT_CHECKPOINTING:\n",
    "            self.encoder.gradient_checkpointing_enable()\n",
    "\n",
    "    def _pool(self, outputs, attention_mask):\n",
    "        if REP_POOLING == \"pooler\" and hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n",
    "            return outputs.pooler_output\n",
    "        if REP_POOLING == \"cls\":\n",
    "            return outputs.last_hidden_state[:, 0]\n",
    "        # default: last4_mean\n",
    "        hs = outputs.hidden_states\n",
    "        last4 = torch.stack(hs[-4:]).mean(dim=0)\n",
    "        return mean_pooling(last4, attention_mask)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                sentiment_labels=None,\n",
    "                polarization_labels=None):\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids if token_type_ids is not None else None,\n",
    "            output_hidden_states=(REP_POOLING != \"pooler\")\n",
    "        )\n",
    "        pooled = self._pool(outputs, attention_mask)\n",
    "        z = self.trunk(pooled)\n",
    "        return {\"logits\": (self.head_sent(z), self.head_pol(z))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3867f2",
   "metadata": {},
   "source": [
    "## SECTION 7: METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade790ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section 7 â€” Metrics =====\n",
    "\n",
    "def compute_metrics_multi(eval_pred):\n",
    "    (sent_logits, pol_logits) = eval_pred.predictions\n",
    "    (y_sent, y_pol) = eval_pred.label_ids\n",
    "\n",
    "    ps = np.argmax(sent_logits, axis=1)\n",
    "    pp = np.argmax(pol_logits, axis=1)\n",
    "\n",
    "    sent_report = classification_report(y_sent, ps, output_dict=True, zero_division=0)\n",
    "    pol_report  = classification_report(y_pol,  pp, output_dict=True, zero_division=0)\n",
    "\n",
    "    sent_f1 = sent_report[\"macro avg\"][\"f1-score\"]\n",
    "    pol_f1  = pol_report[\"macro avg\"][\"f1-score\"]\n",
    "    macro_f1_avg = (sent_f1 + pol_f1) / 2.0\n",
    "\n",
    "    return {\n",
    "        \"sent_acc\": sent_report[\"accuracy\"],\n",
    "        \"sent_prec\": sent_report[\"macro avg\"][\"precision\"],\n",
    "        \"sent_rec\": sent_report[\"macro avg\"][\"recall\"],\n",
    "        \"sent_f1\": sent_f1,\n",
    "\n",
    "        \"pol_acc\": pol_report[\"accuracy\"],\n",
    "        \"pol_prec\": pol_report[\"macro avg\"][\"precision\"],\n",
    "        \"pol_rec\": pol_report[\"macro avg\"][\"recall\"],\n",
    "        \"pol_f1\": pol_f1,\n",
    "\n",
    "        \"macro_f1_avg\": macro_f1_avg\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9dee97",
   "metadata": {},
   "source": [
    "## SECTION 8: CUSTOM TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba73e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section 8 â€” Custom Trainer (R-Drop + LLRD) =====\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, gamma=2.0, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    def forward(self, logits, target):\n",
    "        logp = F.log_softmax(logits, dim=1)\n",
    "        p = torch.exp(logp)\n",
    "        loss = F.nll_loss((1 - p) ** self.gamma * logp, target, weight=self.weight, reduction=\"none\")\n",
    "        return loss.mean() if self.reduction == \"mean\" else loss.sum()\n",
    "\n",
    "def _sym_kl_with_logits(logits1, logits2):\n",
    "    p = F.log_softmax(logits1, dim=-1);  q = F.log_softmax(logits2, dim=-1)\n",
    "    p_exp, q_exp = p.exp(), q.exp()\n",
    "    return 0.5 * (F.kl_div(p, q_exp, reduction=\"batchmean\") + F.kl_div(q, p_exp, reduction=\"batchmean\"))\n",
    "\n",
    "class MultiTaskTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, task_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights or {}\n",
    "        self.task_weights  = task_weights or {\"sentiment\": 1.0, \"polarization\": 1.0}\n",
    "        self._custom_train_sampler = None\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        if self.optimizer is not None:\n",
    "            return self.optimizer\n",
    "        if not USE_LLRD:\n",
    "            self.optimizer = AdamW(self.get_decay_parameter_groups(self.model), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "            return self.optimizer\n",
    "\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
    "        encoder = self.model.encoder\n",
    "        n_layers = getattr(encoder.config, \"num_hidden_layers\", 12)\n",
    "        layers = getattr(getattr(encoder, \"encoder\", encoder), \"layer\", None)\n",
    "        if layers is None:\n",
    "            self.optimizer = AdamW(self.get_decay_parameter_groups(self.model), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "            return self.optimizer\n",
    "\n",
    "        param_groups = []\n",
    "\n",
    "        # Embeddings\n",
    "        emb = getattr(encoder, \"embeddings\", None)\n",
    "        if emb is not None:\n",
    "            lr_emb = LR * (LLRD_DECAY ** n_layers)\n",
    "            decay, nodecay = [], []\n",
    "            for n, p in emb.named_parameters():\n",
    "                (nodecay if any(nd in n for nd in no_decay) else decay).append(p)\n",
    "            if decay:   param_groups.append({\"params\": decay,   \"lr\": lr_emb, \"weight_decay\": WEIGHT_DECAY})\n",
    "            if nodecay: param_groups.append({\"params\": nodecay, \"lr\": lr_emb, \"weight_decay\": 0.0})\n",
    "\n",
    "        # Encoder blocks\n",
    "        for i in range(n_layers):\n",
    "            block = layers[i]\n",
    "            lr_i = LR * (LLRD_DECAY ** (n_layers - 1 - i))\n",
    "            decay, nodecay = [], []\n",
    "            for n, p in block.named_parameters():\n",
    "                (nodecay if any(nd in n for nd in no_decay) else decay).append(p)\n",
    "            if decay:   param_groups.append({\"params\": decay,   \"lr\": lr_i, \"weight_decay\": WEIGHT_DECAY})\n",
    "            if nodecay: param_groups.append({\"params\": nodecay, \"lr\": lr_i, \"weight_decay\": 0.0})\n",
    "\n",
    "        # Pooler\n",
    "        pooler = getattr(encoder, \"pooler\", None)\n",
    "        if pooler is not None:\n",
    "            decay, nodecay = [], []\n",
    "            for n, p in pooler.named_parameters():\n",
    "                (nodecay if any(nd in n for nd in no_decay) else decay).append(p)\n",
    "            if decay:   param_groups.append({\"params\": decay,   \"lr\": LR, \"weight_decay\": WEIGHT_DECAY})\n",
    "            if nodecay: param_groups.append({\"params\": nodecay, \"lr\": LR, \"weight_decay\": 0.0})\n",
    "\n",
    "        # Heads/trunk (highest LR)\n",
    "        head_lr = LR * HEAD_LR_MULT\n",
    "        head_modules = [self.model.trunk, self.model.head_sent, self.model.head_pol]\n",
    "        decay, nodecay = [], []\n",
    "        for m in head_modules:\n",
    "            for n, p in m.named_parameters():\n",
    "                (nodecay if any(nd in n for nd in no_decay) else decay).append(p)\n",
    "        if decay:   param_groups.append({\"params\": decay,   \"lr\": head_lr, \"weight_decay\": WEIGHT_DECAY})\n",
    "        if nodecay: param_groups.append({\"params\": nodecay, \"lr\": head_lr, \"weight_decay\": 0.0})\n",
    "\n",
    "        self.optimizer = AdamW(param_groups, lr=LR)\n",
    "        return self.optimizer\n",
    "\n",
    "    def set_train_sampler(self, sampler):\n",
    "        self._custom_train_sampler = sampler\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        if self.train_dataset is None:\n",
    "            return None\n",
    "        if self._custom_train_sampler is not None:\n",
    "            return DataLoader(\n",
    "                self.train_dataset,\n",
    "                batch_size=self.args.train_batch_size,\n",
    "                sampler=self._custom_train_sampler,\n",
    "                collate_fn=self.data_collator,\n",
    "                drop_last=self.args.dataloader_drop_last,\n",
    "                num_workers=self.args.dataloader_num_workers,\n",
    "                pin_memory=self.args.dataloader_pin_memory,\n",
    "            )\n",
    "        return super().get_train_dataloader()\n",
    "\n",
    "    def _sent_loss_fn(self, weight, logits, target):\n",
    "        if USE_FOCAL_SENTIMENT:\n",
    "            return FocalLoss(weight=weight, gamma=FOCAL_GAMMA_SENTIMENT)(logits, target)\n",
    "        return nn.CrossEntropyLoss(weight=weight, label_smoothing=float(LABEL_SMOOTH_SENTIMENT))(logits, target)\n",
    "\n",
    "    def _pol_loss_fn(self, weight, logits, target):\n",
    "        if USE_FOCAL_POLARITY:\n",
    "            return FocalLoss(weight=weight, gamma=FOCAL_GAMMA_POLARITY)(logits, target)\n",
    "        return nn.CrossEntropyLoss(weight=weight, label_smoothing=float(LABEL_SMOOTH_POLARITY))(logits, target)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        y_sent = inputs.pop(\"sentiment_labels\")\n",
    "        y_pol  = inputs.pop(\"polarization_labels\")\n",
    "\n",
    "        current_epoch = getattr(self.state, 'epoch', 0) if hasattr(self, 'state') else 0\n",
    "        use_rdrop_now = USE_RDROP and model.training and current_epoch >= RDROP_WARMUP_EPOCHS\n",
    "\n",
    "        if use_rdrop_now:\n",
    "            outputs1 = model(**inputs)\n",
    "            outputs2 = model(**inputs)\n",
    "            s1, p1 = outputs1[\"logits\"]\n",
    "            s2, p2 = outputs2[\"logits\"]\n",
    "\n",
    "            ws = self.class_weights.get(\"sentiment\", None); ws = ws.to(s1.device) if ws is not None else None\n",
    "            wp = self.class_weights.get(\"polarization\", None); wp = wp.to(p1.device) if wp is not None else None\n",
    "\n",
    "            ce_s = 0.5 * (self._sent_loss_fn(ws, s1, y_sent) + self._sent_loss_fn(ws, s2, y_sent))\n",
    "            ce_p = 0.5 * (self._pol_loss_fn(wp,  p1, y_pol)  + self._pol_loss_fn(wp,  p2, y_pol))\n",
    "            kl_s = _sym_kl_with_logits(s1, s2)\n",
    "            kl_p = _sym_kl_with_logits(p1, p2)\n",
    "\n",
    "            w_s = float(self.task_weights.get(\"sentiment\", 1.0))\n",
    "            w_p = float(self.task_weights.get(\"polarization\", 1.0))\n",
    "\n",
    "            rdrop_factor = min(1.0, (current_epoch - RDROP_WARMUP_EPOCHS + 1) / 2.0)\n",
    "            loss = w_s * ce_s + w_p * ce_p + (RDROP_ALPHA * rdrop_factor) * (kl_s + kl_p)\n",
    "            if return_outputs:\n",
    "                return loss, {\"logits\": (s1, p1)}\n",
    "            return loss\n",
    "\n",
    "        # Standard single forward\n",
    "        outputs = model(**inputs)\n",
    "        s, p = outputs[\"logits\"]\n",
    "\n",
    "        ws = self.class_weights.get(\"sentiment\", None); ws = ws.to(s.device) if ws is not None else None\n",
    "        wp = self.class_weights.get(\"polarization\", None); wp = wp.to(p.device) if wp is not None else None\n",
    "\n",
    "        loss_s = self._sent_loss_fn(ws, s, y_sent)\n",
    "        loss_p = self._pol_loss_fn(wp, p, y_pol)\n",
    "\n",
    "        w_s = float(self.task_weights.get(\"sentiment\", 1.0))\n",
    "        w_p = float(self.task_weights.get(\"polarization\", 1.0))\n",
    "        loss = w_s * loss_s + w_p * loss_p\n",
    "\n",
    "        if return_outputs:\n",
    "            outputs = dict(outputs); outputs[\"labels\"] = (y_sent, y_pol)\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        y_sent = inputs.get(\"sentiment_labels\", None)\n",
    "        y_pol  = inputs.get(\"polarization_labels\", None)\n",
    "\n",
    "        model_inputs = {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]}\n",
    "        if \"token_type_ids\" in inputs:\n",
    "            model_inputs[\"token_type_ids\"] = inputs[\"token_type_ids\"]\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**model_inputs)\n",
    "            s, p = outputs[\"logits\"]\n",
    "\n",
    "        loss = None\n",
    "        logits = (s.detach(), p.detach())\n",
    "        labels = (y_sent, y_pol) if isinstance(y_sent, torch.Tensor) and isinstance(y_pol, torch.Tensor) else None\n",
    "        return (loss, logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e4e22",
   "metadata": {},
   "source": [
    "## SECTION 9: TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f72d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section 9 â€” Train/Evaluate One Model =====\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "def train_eval_one_model(model_key: str,\n",
    "                         X_tr: pd.DataFrame, X_v: pd.DataFrame, X_te: pd.DataFrame,\n",
    "                         ysent_tr: np.ndarray, ysent_v: np.ndarray, ysent_te: np.ndarray,\n",
    "                         ypol_tr: np.ndarray,  ypol_v: np.ndarray,  ypol_te: np.ndarray,\n",
    "                         sent_w_np: np.ndarray, pol_w_np: np.ndarray):\n",
    "    base_name = MODEL_CONFIGS[model_key][\"name\"]\n",
    "    run_dir = os.path.join(OUT_DIR, f\"{model_key}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_name)\n",
    "    collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "    tr_titles, tr_texts = X_tr[TITLE_COL].values, X_tr[TEXT_COL].values\n",
    "    v_titles,  v_texts  = X_v[TITLE_COL].values, X_v[TEXT_COL].values\n",
    "    te_titles, te_texts = X_te[TITLE_COL].values, X_te[TEXT_COL].values\n",
    "\n",
    "    train_ds = TaglishDataset(tr_titles, tr_texts, ysent_tr, ypol_tr, tokenizer, max_length=MAX_LENGTH)\n",
    "    val_ds   = TaglishDataset(v_titles,  v_texts,  ysent_v,  ypol_v,  tokenizer, max_length=MAX_LENGTH)\n",
    "    test_ds  = TaglishDataset(te_titles, te_texts, ysent_te, ypol_te, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "    model = MultiTaskModel(base_name, num_sent_classes, num_pol_classes).to(device)\n",
    "\n",
    "    sent_w = torch.tensor(sent_w_np, dtype=torch.float32)\n",
    "    pol_w  = torch.tensor(pol_w_np,  dtype=torch.float32)\n",
    "\n",
    "    args = make_training_args_compat(\n",
    "        output_dir=run_dir,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "        lr_scheduler_kwargs={\"num_cycles\": NUM_CYCLES},\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1_avg\",\n",
    "        greater_is_better=True,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_dir=os.path.join(run_dir, \"logs\"),\n",
    "        logging_steps=25,\n",
    "        logging_first_step=True,\n",
    "        save_steps=500,\n",
    "        eval_steps=None,\n",
    "        report_to=\"none\",\n",
    "        seed=42,\n",
    "        remove_unused_columns=False,\n",
    "        eval_accumulation_steps=1,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=0,  # Windows compatibility\n",
    "        max_grad_norm=MAX_GRAD_NORM,\n",
    "        label_smoothing_factor=0.0,\n",
    "        save_total_limit=3,\n",
    "        prediction_loss_only=False\n",
    "    )\n",
    "\n",
    "    callbacks = get_early_stopping_callbacks(EARLY_STOP_PATIENCE)\n",
    "\n",
    "    trainer = MultiTaskTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_multi,\n",
    "        callbacks=callbacks,\n",
    "        class_weights={\"sentiment\": sent_w, \"polarization\": pol_w},\n",
    "        task_weights=TASK_LOSS_WEIGHTS\n",
    "    )\n",
    "\n",
    "    # Enhanced joint oversampling with objective + neutral boost\n",
    "    if USE_OVERSAMPLING and USE_JOINT_OVERSAMPLING:\n",
    "        pair_counts = Counter(zip(ysent_tr.tolist(), ypol_tr.tolist()))\n",
    "        counts = np.array(list(pair_counts.values()), dtype=np.float32)\n",
    "        med = float(np.median(counts)) if len(counts) else 1.0\n",
    "\n",
    "        obj_idx = np.where(pol_le.classes_ == \"objective\")[0][0] if \"objective\" in pol_le.classes_ else 1\n",
    "        neutral_idx = np.where(sent_le.classes_ == \"neutral\")[0][0] if \"neutral\" in sent_le.classes_ else 1\n",
    "\n",
    "        def inv_mult(c):\n",
    "            if c <= 0: return JOINT_OVERSAMPLING_MAX_MULT\n",
    "            return float(np.clip(med / float(c), 1.0, JOINT_OVERSAMPLING_MAX_MULT))\n",
    "\n",
    "        inv_by_pair = {k: inv_mult(v) for k, v in pair_counts.items()}\n",
    "        sample_weights = []\n",
    "\n",
    "        for ys, yp in zip(ysent_tr, ypol_tr):\n",
    "            inv = inv_by_pair.get((int(ys), int(yp)), 1.0)\n",
    "            w = (1.0 - JOINT_ALPHA) * 1.0 + JOINT_ALPHA * inv\n",
    "\n",
    "            if USE_SMART_OVERSAMPLING and int(yp) == obj_idx:\n",
    "                w *= OBJECTIVE_BOOST_MULT\n",
    "            if USE_SMART_OVERSAMPLING and int(ys) == neutral_idx:\n",
    "                w *= NEUTRAL_BOOST_MULT\n",
    "\n",
    "            sample_weights.append(w)\n",
    "\n",
    "        obj_boost_count = sum(1 for i, yp in enumerate(ypol_tr) if int(yp) == obj_idx and sample_weights[i] > 2.0)\n",
    "        neutral_boost_count = sum(1 for i, ys in enumerate(ysent_tr) if int(ys) == neutral_idx and sample_weights[i] > 2.0)\n",
    "        print(f\"ðŸ”¥ Enhanced Oversampling: min={min(sample_weights):.2f}, max={max(sample_weights):.2f}\")\n",
    "        print(f\"   â”œâ”€ Objective boosted samples: {obj_boost_count}\")\n",
    "        print(f\"   â””â”€ Neutral boosted samples: {neutral_boost_count}\")\n",
    "        trainer.set_train_sampler(WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True))\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Test\n",
    "    test_out = trainer.predict(test_ds)\n",
    "    metrics = {f\"test_{k}\": float(v) for k, v in test_out.metrics.items()}\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # Ensure weights exist\n",
    "    model_path = os.path.join(run_dir, \"pytorch_model.bin\")\n",
    "    if not os.path.exists(model_path):\n",
    "        torch.save(trainer.model.state_dict(), model_path)\n",
    "    tokenizer.save_pretrained(run_dir)\n",
    "    \n",
    "    with open(os.path.join(run_dir, \"metrics_test.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    sent_logits, pol_logits = test_out.predictions\n",
    "    ysent_pred = np.argmax(sent_logits, axis=1)\n",
    "    ypol_pred  = np.argmax(pol_logits,  axis=1)\n",
    "\n",
    "    cm_sent = confusion_matrix(ysent_te, ysent_pred, labels=list(range(num_sent_classes)))\n",
    "    cm_pol  = confusion_matrix(ypol_te,  ypol_pred,  labels=list(range(num_pol_classes)))\n",
    "    np.save(os.path.join(run_dir, \"cm_sent.npy\"), cm_sent)\n",
    "    np.save(os.path.join(run_dir, \"cm_pol.npy\"),  cm_pol)\n",
    "\n",
    "    def plot_cm(cm, labels, title, path_png):\n",
    "        fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "        im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "        ax.set_title(title); ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "        ax.set_xticks(range(len(labels))); ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "        ax.set_yticks(range(len(labels))); ax.set_yticklabels(labels)\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "        fig.colorbar(im, ax=ax, fraction=0.046); plt.tight_layout(); plt.savefig(path_png, dpi=160); plt.close(fig)\n",
    "\n",
    "    plot_cm(cm_sent, sent_le.classes_, \"Sentiment Confusion\", os.path.join(run_dir, \"cm_sent.png\"))\n",
    "    plot_cm(cm_pol,  pol_le.classes_,  \"Polarization Confusion\", os.path.join(run_dir, \"cm_pol.png\"))\n",
    "\n",
    "    rep_sent = classification_report(ysent_te, ysent_pred, target_names=sent_le.classes_, digits=4, zero_division=0)\n",
    "    rep_pol  = classification_report(ypol_te,  ypol_pred,  target_names=pol_le.classes_,  digits=4, zero_division=0)\n",
    "    with open(os.path.join(run_dir, \"report_sentiment.txt\"), \"w\") as f: f.write(rep_sent)\n",
    "    with open(os.path.join(run_dir, \"report_polarization.txt\"), \"w\") as f: f.write(rep_pol)\n",
    "\n",
    "    return {\"model_key\": model_key, \"base_name\": base_name, **metrics}, (ysent_pred, ypol_pred)\n",
    "\n",
    "timer.end_section(\"SECTION 5-9: Model Architecture & Training Setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfefab",
   "metadata": {},
   "source": [
    "## SECTION 10: RUN TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5822d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section 10 â€” Run Training =====\n",
    "\n",
    "timer.start_section(\"SECTION 10: Model Training Execution\")\n",
    "\n",
    "results = []\n",
    "pred_cache = {}\n",
    "\n",
    "for key in MODELS_TO_RUN:\n",
    "    print(f\"\\n=== Running {key} -> {MODEL_CONFIGS[key]['name']} ===\")\n",
    "    row, preds = train_eval_one_model(\n",
    "        key,\n",
    "        X_train, X_val, X_test,\n",
    "        ysent_train, ysent_val, ysent_test,\n",
    "        ypol_train,  ypol_val,  ypol_test,\n",
    "        sent_weights_np, pol_weights_np\n",
    "    )\n",
    "    results.append(row)\n",
    "    pred_cache[key] = preds\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(OUT_DIR, \"summary_results.csv\"), index=False)\n",
    "\n",
    "timer.end_section(\"SECTION 10: Model Training Execution\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ‰ TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a4ca7",
   "metadata": {},
   "source": [
    "## SECTION 11: DETAILED ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfaec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section 11 â€” Detailed Breakdown Reports =====\n",
    "\n",
    "DETAILS_DIR = os.path.join(OUT_DIR, \"details\")\n",
    "os.makedirs(DETAILS_DIR, exist_ok=True)\n",
    "\n",
    "def per_class_breakdown(y_true, y_pred, class_names):\n",
    "    rep = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=list(class_names),\n",
    "        output_dict=True, zero_division=0\n",
    "    )\n",
    "    rows = []\n",
    "    for cname in class_names:\n",
    "        if cname in rep:\n",
    "            rows.append({\n",
    "                \"class\": cname,\n",
    "                \"precision\": rep[cname][\"precision\"],\n",
    "                \"recall\":    rep[cname][\"recall\"],\n",
    "                \"f1\":        rep[cname][\"f1-score\"],\n",
    "                \"support\":   int(rep[cname][\"support\"]),\n",
    "            })\n",
    "        else:\n",
    "            rows.append({\"class\": cname, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"support\": 0})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "all_breakdowns = {}\n",
    "\n",
    "for key in MODELS_TO_RUN:\n",
    "    print(f\"\\n=== Detailed breakdowns for {key} ===\")\n",
    "    ysent_pred, ypol_pred = pred_cache[key]\n",
    "\n",
    "    sent_per_class = per_class_breakdown(ysent_test, ysent_pred, sent_le.classes_)\n",
    "    pol_per_class  = per_class_breakdown(ypol_test,  ypol_pred,  pol_le.classes_)\n",
    "\n",
    "    sent_csv = os.path.join(DETAILS_DIR, f\"{key}_sentiment_per_class.csv\")\n",
    "    pol_csv  = os.path.join(DETAILS_DIR, f\"{key}_polarization_per_class.csv\")\n",
    "    sent_per_class.to_csv(sent_csv, index=False)\n",
    "    pol_per_class.to_csv(pol_csv, index=False)\n",
    "\n",
    "    print(\"\\nSentiment â€” per class:\")\n",
    "    display(sent_per_class)\n",
    "\n",
    "    print(\"\\nPolarization â€” per class:\")\n",
    "    display(pol_per_class)\n",
    "\n",
    "    all_breakdowns[key] = {\n",
    "        \"sentiment_per_class_csv\": sent_csv,\n",
    "        \"polarization_per_class_csv\": pol_csv,\n",
    "    }\n",
    "\n",
    "with open(os.path.join(DETAILS_DIR, \"index.json\"), \"w\") as f:\n",
    "    json.dump(all_breakdowns, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved detailed breakdowns to:\", DETAILS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb61cf",
   "metadata": {},
   "source": [
    "## SECTION 12: FINAL SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a565d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section 12 â€” Final Summary =====\n",
    "\n",
    "timer.get_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š REMBERT RUN #1 - FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: google/rembert\")\n",
    "print(f\"Hardware: RTX 3060 (12GB VRAM)\")\n",
    "print(f\"Dataset: {len(df)} samples (augmented)\")\n",
    "print(f\"Train/Val/Test: {len(X_train)}/{len(X_val)}/{len(X_test)}\")\n",
    "print()\n",
    "print(\"Results:\")\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"  Overall Macro-F1: {row.get('test_macro_f1_avg', 0)*100:.2f}%\")\n",
    "    print(f\"  Sentiment F1: {row.get('test_sent_f1', 0)*100:.2f}%\")\n",
    "    print(f\"  Polarization F1: {row.get('test_pol_f1', 0)*100:.2f}%\")\n",
    "print()\n",
    "print(f\"Output directory: {OUT_DIR}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
