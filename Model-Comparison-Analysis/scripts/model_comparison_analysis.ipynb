{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4809751d",
   "metadata": {},
   "source": [
    "# Model Performance Comparison Analysis\n",
    "\n",
    "Interactive comparison of mBERT, XLM-RoBERTa, and RemBERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd72bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"âœ“ Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb60ae",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance data\n",
    "data = {\n",
    "    'Model': ['mBERT', 'XLM-R', 'RemBERT'],\n",
    "    'sent_acc': [0.981633, 0.963776, 0.975420],\n",
    "    'sent_prec': [0.983863, 0.970607, 0.978235],\n",
    "    'sent_rec': [0.986341, 0.973066, 0.980156],\n",
    "    'sent_f1': [0.985077, 0.971761, 0.979142],\n",
    "    'pol_acc': [0.845918, 0.870408, 0.858210],\n",
    "    'pol_prec': [0.820192, 0.840429, 0.832145],\n",
    "    'pol_rec': [0.893018, 0.902585, 0.897850],\n",
    "    'pol_f1': [0.846428, 0.866122, 0.857320],\n",
    "    'macro_f1_avg': [0.915752, 0.918941, 0.917231],\n",
    "    'runtime': [4.2996, 4.423, 16.8042],\n",
    "    'training_time': ['1.9h 52m', '2.5h 30m', '9.5h 31m'],\n",
    "    'training_mins': [112, 150, 571]  # in minutes\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88723555",
   "metadata": {},
   "source": [
    "## 2. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf3ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display key metrics\n",
    "display_cols = ['Model', 'sent_f1', 'pol_f1', 'macro_f1_avg', 'training_time']\n",
    "df[display_cols].style.background_gradient(subset=['sent_f1', 'pol_f1', 'macro_f1_avg'], cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6762af2b",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d9173",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Sentiment Analysis Metrics Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['sent_acc', 'sent_prec', 'sent_rec', 'sent_f1']\n",
    "titles = ['Sentiment Accuracy', 'Sentiment Precision', 'Sentiment Recall', 'Sentiment F1']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    bars = ax.bar(df['Model'], df[metric], color=colors)\n",
    "    ax.set_ylabel('Score', fontweight='bold')\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    ax.set_ylim([0.94, 1.0])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.4f}',\n",
    "               ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3ba4d3",
   "metadata": {},
   "source": [
    "## 4. Polarization Detection Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdead39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Polarization Detection Metrics Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['pol_acc', 'pol_prec', 'pol_rec', 'pol_f1', 'macro_f1_avg']\n",
    "titles = ['Polarization Accuracy', 'Polarization Precision', 'Polarization Recall', \n",
    "          'Polarization F1', 'Macro F1 Average']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    bars = ax.bar(df['Model'], df[metric], color=colors)\n",
    "    ax.set_ylabel('Score', fontweight='bold')\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    ax.set_ylim([0.80, 1.0])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.4f}',\n",
    "               ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "axes[1, 2].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0977c5e7",
   "metadata": {},
   "source": [
    "## 5. Overall Performance Radar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bdcff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Sentiment F1', 'Polarization F1', 'Sent Accuracy', 'Pol Accuracy', 'Macro F1']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "N = len(categories)\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "ax.set_theta_offset(np.pi / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=11)\n",
    "ax.set_ylim(0.80, 1.0)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    values = [row['sent_f1'], row['pol_f1'], row['sent_acc'], row['pol_acc'], row['macro_f1_avg']]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[idx])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12)\n",
    "ax.set_title('Overall Model Performance Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa074f4",
   "metadata": {},
   "source": [
    "## 6. Training Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2760e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    ax.scatter(row['training_mins'], row['macro_f1_avg'], s=500, \n",
    "              label=row['Model'], alpha=0.6, color=colors[idx])\n",
    "    ax.annotate(row['Model'], (row['training_mins'], row['macro_f1_avg']),\n",
    "               fontsize=12, fontweight='bold',\n",
    "               xytext=(10, 10), textcoords='offset points')\n",
    "\n",
    "ax.set_xlabel('Training Time (minutes)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Macro F1 Average', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Training Efficiency: Performance vs Time', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd33d70",
   "metadata": {},
   "source": [
    "## 7. Metrics Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd787c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for heatmap\n",
    "heatmap_data = df[['Model', 'sent_acc', 'sent_prec', 'sent_rec', 'sent_f1',\n",
    "                    'pol_acc', 'pol_prec', 'pol_rec', 'pol_f1', 'macro_f1_avg']].set_index('Model')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "sns.heatmap(heatmap_data.T, annot=True, fmt='.4f', cmap='RdYlGn', \n",
    "           center=0.9, vmin=0.80, vmax=1.0, ax=ax,\n",
    "           cbar_kws={'label': 'Score'})\n",
    "\n",
    "ax.set_title('Performance Metrics Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Models', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Metrics', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1565749f",
   "metadata": {},
   "source": [
    "## 8. Best Model Per Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_check = ['sent_acc', 'sent_f1', 'pol_acc', 'pol_f1', 'macro_f1_avg']\n",
    "best_models = {}\n",
    "\n",
    "for metric in metrics_to_check:\n",
    "    best_idx = df[metric].idxmax()\n",
    "    best_model = df.loc[best_idx, 'Model']\n",
    "    best_score = df.loc[best_idx, metric]\n",
    "    best_models[metric] = {'model': best_model, 'score': best_score}\n",
    "\n",
    "best_df = pd.DataFrame(best_models).T\n",
    "best_df.columns = ['Best Model', 'Score']\n",
    "best_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499aa99c",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **XLM-R** achieves the best overall performance\n",
    "   - Macro F1: **91.89%**\n",
    "   - Polarization F1: **86.61%**\n",
    "   - Moderate training time: **2.5 hours**\n",
    "   - Best balance of performance and efficiency\n",
    "\n",
    "2. **mBERT** excels in sentiment analysis\n",
    "   - Sentiment F1: **98.51%**\n",
    "   - Fastest training time: **1.9 hours**\n",
    "   - Strong overall performance: 91.58% macro F1\n",
    "\n",
    "3. **RemBERT** performs between mBERT and XLM-R\n",
    "   - Sentiment F1: **97.91%**\n",
    "   - Polarization F1: **85.73%**\n",
    "   - Macro F1: **91.72%**\n",
    "   - Longest training time (9.5 hours) without top performance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
