{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 1: ENV + VERSIONS"
      ],
      "metadata": {
        "id": "QH8tfwdRSfXL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYn5N2A6SVNo",
        "outputId": "e0a5b887-aaa8-4415-fa62-1c12cbd1587b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SECTION 1: ENVIRONMENT SETUP (COLAB-FRIENDLY, LIGHTWEIGHT)\n",
        "# ============================================================================\n",
        "import sys, subprocess, importlib, os, random\n",
        "\n",
        "def pipi(*pkgs):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"--no-cache-dir\", *pkgs])\n",
        "\n",
        "# Pin core libs (align with your mBERT/XLM-R stacks)\n",
        "pipi(\n",
        "    \"numpy==2.1.1\",\n",
        "    \"pandas==2.2.3\",\n",
        "    \"scikit-learn==1.5.2\",\n",
        "    \"matplotlib==3.9.2\",\n",
        "    \"transformers==4.44.2\",\n",
        "    \"accelerate==0.34.2\",\n",
        "    \"bitsandbytes\",\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "import torch, transformers, pandas as pd\n",
        "# Speed/compat env â€“ reduce fragmentation and disable W&B\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Enable TF32 on Ampere+ (big win with minimal accuracy impact)\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "    except Exception:\n",
        "        pass\n",
        "print(\"CUDA:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 2"
      ],
      "metadata": {
        "id": "4EAgXY8TXVuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 2: CONFIG + SEED (REM BERT BASELINE)\n",
        "# ============================================================================\n",
        "from transformers import set_seed\n",
        "\n",
        "# IO\n",
        "OUT_DIR = \"./runs_rembert_optimized\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Data\n",
        "CSV_PATH     = \"/content/adjudications_2025-10-22.csv\"              # base\n",
        "AUG_CSV_PATH = \"/content/augmented_adjudications_2025-10-22.csv\"    # augmented\n",
        "USE_AUGMENTED_TRAIN = True  # append augmentation to TRAIN only\n",
        "\n",
        "# Columns (mirror mBERT notebook)\n",
        "TITLE_COL = \"Title\"\n",
        "TEXT_COL  = \"Comment\"\n",
        "SENT_COL  = \"Final Sentiment\"\n",
        "POL_COL   = \"Final Polarization\"\n",
        "\n",
        "# Model\n",
        "MODEL_NAME = \"google/rembert\"\n",
        "MAX_LENGTH = 256  # reduce seq length to cut memory/compute (~0.64x attention cost)\n",
        "USE_GRADIENT_CHECKPOINTING = True\n",
        "\n",
        "# Train\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 12\n",
        "GRAD_ACCUM_STEPS = 3\n",
        "LR = 2.0e-5              # safe starting LR for RemBERT\n",
        "WARMUP_RATIO = 0.20\n",
        "WEIGHT_DECAY = 0.03\n",
        "EARLY_STOP_PATIENCE = 8\n",
        "MAX_GRAD_NORM = 0.5\n",
        "\n",
        "# Heads / pooling (kept simple; mirrors your Run 16)\n",
        "HEAD_HIDDEN = 768  # lighten heads to reduce optimizer state and memory\n",
        "HEAD_LAYERS = 3\n",
        "HEAD_DROPOUT = 0.28\n",
        "REP_POOLING = \"last4_mean\"\n",
        "HEAD_LR_MULT = 3.0\n",
        "\n",
        "# Loss / task weights\n",
        "FOCAL_GAMMA_SENTIMENT = 2.5\n",
        "FOCAL_GAMMA_POLARITY  = 3.5\n",
        "LABEL_SMOOTH_SENTIMENT = 0.10\n",
        "LABEL_SMOOTH_POLARITY  = 0.08\n",
        "TASK_LOSS_WEIGHTS = {\"sentiment\": 1.0, \"polarization\": 1.4}\n",
        "\n",
        "# LLRD\n",
        "USE_LLRD = True\n",
        "LLRD_DECAY = 0.90\n",
        "\n",
        "# Seed\n",
        "SEED = 42\n",
        "\n",
        "def seed_all(seed=SEED):\n",
        "    transformers.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_all(SEED)\n",
        "print(f\"Seed set: {SEED}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND5VlO-LV-lE",
        "outputId": "a8721c17-b212-4223-fe7a-67631f4536b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed set: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 3\n"
      ],
      "metadata": {
        "id": "WydtYjMOWByD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 3: DATA LOADING + STRATIFIED SPLIT + AUGMENT APPEND (TRAIN ONLY)\n",
        "# ============================================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load base CSV (split on base only)\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "required = [TITLE_COL, TEXT_COL, SENT_COL, POL_COL]\n",
        "missing = [c for c in required if c not in df.columns]\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing columns: {missing}. Available: {list(df.columns)}\")\n",
        "\n",
        "df = df.dropna(subset=required).reset_index(drop=True)\n",
        "\n",
        "# Encode labels\n",
        "sent_le = LabelEncoder().fit(df[SENT_COL].astype(str))\n",
        "pol_le  = LabelEncoder().fit(df[POL_COL].astype(str))\n",
        "\n",
        "df[\"sent_y\"], df[\"pol_y\"] = sent_le.transform(df[SENT_COL].astype(str)), pol_le.transform(df[POL_COL].astype(str))\n",
        "\n",
        "# Stratify on joint so both tasks are balanced\n",
        "joint = df[\"sent_y\"] * 10 + df[\"pol_y\"]\n",
        "X = df[[TITLE_COL, TEXT_COL]].copy()\n",
        "y_sent = df[\"sent_y\"].values\n",
        "y_pol  = df[\"pol_y\"].values\n",
        "\n",
        "X_train, X_tmp, ysent_train, ysent_tmp, ypol_train, ypol_tmp = train_test_split(\n",
        "    X, y_sent, y_pol, test_size=0.30, random_state=SEED, stratify=joint\n",
        ")\n",
        "joint_tmp = ysent_tmp * 10 + ypol_tmp\n",
        "X_val, X_test, ysent_val, ysent_test, ypol_val, ypol_test = train_test_split(\n",
        "    X_tmp, ysent_tmp, ypol_tmp, test_size=0.50, random_state=SEED, stratify=joint_tmp\n",
        ")\n",
        "\n",
        "# Append augmentation to TRAIN only\n",
        "if USE_AUGMENTED_TRAIN and os.path.isfile(AUG_CSV_PATH):\n",
        "    aug = pd.read_csv(AUG_CSV_PATH).dropna(subset=required)\n",
        "    aug[\"sent_y\"], aug[\"pol_y\"] = sent_le.transform(aug[SENT_COL].astype(str)), pol_le.transform(aug[POL_COL].astype(str))\n",
        "    X_train = pd.concat([X_train, aug[[TITLE_COL, TEXT_COL]]], ignore_index=True)\n",
        "    ysent_train = np.concatenate([ysent_train, aug[\"sent_y\"].values])\n",
        "    ypol_train  = np.concatenate([ypol_train,  aug[\"pol_y\"].values])\n",
        "    print(\"Augmentation appended to TRAIN only. Oversampling disabled.\")\n",
        "else:\n",
        "    print(\"Augmented file missing or disabled; training on base only.\")\n",
        "\n",
        "print(\"Split sizes:\", len(X_train), len(X_val), len(X_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe0CLTMpWBhh",
        "outputId": "0469fd33-2695-4c63-adfb-e3f84316975d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmentation appended to TRAIN only. Oversampling disabled.\n",
            "Split sizes: 20038 1495 1495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 4\n"
      ],
      "metadata": {
        "id": "NVhyOPOGWGD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 4: TOKENIZER + DATASETS\n",
        "# ============================================================================\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "class PairDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, titles, texts, y_sent, y_pol, tok, max_length=320):\n",
        "        self.titles = titles.reset_index(drop=True)\n",
        "        self.texts  = texts.reset_index(drop=True)\n",
        "        self.y_sent = y_sent\n",
        "        self.y_pol  = y_pol\n",
        "        self.tok = tok\n",
        "        self.maxlen = max_length\n",
        "    def __len__(self): return len(self.titles)\n",
        "    def __getitem__(self, i):\n",
        "        enc = self.tok(\n",
        "            str(self.titles.iloc[i]),\n",
        "            str(self.texts.iloc[i]),\n",
        "            truncation=True,\n",
        "            max_length=self.maxlen,\n",
        "            padding=False,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        item[\"sentiment_labels\"]    = torch.tensor(self.y_sent[i], dtype=torch.long)\n",
        "        item[\"polarization_labels\"] = torch.tensor(self.y_pol[i],  dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "train_ds = PairDataset(X_train[TITLE_COL], X_train[TEXT_COL], ysent_train, ypol_train, tokenizer, MAX_LENGTH)\n",
        "val_ds   = PairDataset(X_val[TITLE_COL],   X_val[TEXT_COL],   ysent_val,   ypol_val,   tokenizer, MAX_LENGTH)\n",
        "test_ds  = PairDataset(X_test[TITLE_COL],  X_test[TEXT_COL],  ysent_test,  ypol_test,  tokenizer, MAX_LENGTH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymK8ZR2vWF29",
        "outputId": "85026b47-2f21-46e5-ab79-1f6979dc2563"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 5\n"
      ],
      "metadata": {
        "id": "Ms38TeFxWJmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 5: MODEL (RemBERT backbone + simple multi-task heads)\n",
        "# ============================================================================\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "\n",
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(self, base_model: str, num_sent: int, num_pol: int):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(base_model)\n",
        "        if USE_GRADIENT_CHECKPOINTING:\n",
        "            self.encoder.gradient_checkpointing_enable()\n",
        "        self.hidden = self.encoder.config.hidden_size  # RemBERT: 1152\n",
        "        # trunk\n",
        "        self.trunk = nn.Sequential(\n",
        "            nn.Linear(self.hidden, HEAD_HIDDEN),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(HEAD_HIDDEN),\n",
        "            nn.Dropout(HEAD_DROPOUT),\n",
        "        )\n",
        "        # heads\n",
        "        def head_block(out_dim: int):\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(HEAD_HIDDEN, HEAD_HIDDEN // 2), nn.GELU(), nn.LayerNorm(HEAD_HIDDEN // 2), nn.Dropout(HEAD_DROPOUT*0.8),\n",
        "                nn.Linear(HEAD_HIDDEN // 2, HEAD_HIDDEN // 4), nn.GELU(), nn.LayerNorm(HEAD_HIDDEN // 4), nn.Dropout(HEAD_DROPOUT*0.7),\n",
        "                nn.Linear(HEAD_HIDDEN // 4, out_dim),\n",
        "            )\n",
        "        self.head_sent = head_block(num_sent)\n",
        "        self.head_pol  = head_block(num_pol)\n",
        "\n",
        "    def _rep(self, outputs, attention_mask):\n",
        "        # last4_mean pooling\n",
        "        hs = outputs.hidden_states  # tuple\n",
        "        last4 = torch.stack(hs[-4:]).mean(0)  # [B,T,H]\n",
        "        mask = attention_mask.unsqueeze(-1)\n",
        "        return (last4 * mask).sum(1) / mask.sum(1).clamp(min=1)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
        "                sentiment_labels=None, polarization_labels=None):\n",
        "        out = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids if token_type_ids is not None else None,\n",
        "            output_hidden_states=True,\n",
        "        )\n",
        "        x = self._rep(out, attention_mask)\n",
        "        x = self.trunk(x)\n",
        "        return {\"logits_sent\": self.head_sent(x), \"logits_pol\": self.head_pol(x)}\n"
      ],
      "metadata": {
        "id": "dCY7YDmgWK7N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 6\n"
      ],
      "metadata": {
        "id": "il5YiA1yWM5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 6: LOSS + TRAINER\n",
        "# ============================================================================\n",
        "import torch.nn.functional as F\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorWithPadding\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, weight=None, gamma=2.0, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.weight, self.gamma, self.reduction = weight, gamma, reduction\n",
        "    def forward(self, logits, target):\n",
        "        logp = F.log_softmax(logits, dim=-1)\n",
        "        p = logp.exp()\n",
        "        loss = F.nll_loss(((1 - p) ** self.gamma) * logp, target, weight=self.weight, reduction=\"none\")\n",
        "        return loss.mean() if self.reduction == \"mean\" else loss.sum() if self.reduction == \"sum\" else loss\n",
        "\n",
        "class MTTrainer(Trainer):\n",
        "    def __init__(self, task_weights=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.task_weights = task_weights or {\"sentiment\": 1.0, \"polarization\": 1.0}\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels_s = inputs.pop(\"sentiment_labels\"); labels_p = inputs.pop(\"polarization_labels\")\n",
        "        outputs = model(**inputs)\n",
        "        ls, lp = outputs[\"logits_sent\"], outputs[\"logits_pol\"]\n",
        "        loss_s = FocalLoss(gamma=FOCAL_GAMMA_SENTIMENT)(ls, labels_s)\n",
        "        loss_p = FocalLoss(gamma=FOCAL_GAMMA_POLARITY)(lp, labels_p)\n",
        "        loss = self.task_weights[\"sentiment\"]*loss_s + self.task_weights[\"polarization\"]*loss_p\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "num_sent, num_pol = int(df[SENT_COL].nunique()), int(df[POL_COL].nunique())\n",
        "model = MultiTaskModel(MODEL_NAME, num_sent=num_sent, num_pol=num_pol)\n",
        "\n",
        "# Save memory and speed: avoid caching past key/values during training\n",
        "try:\n",
        "    model.encoder.config.use_cache = False\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Optionally freeze embeddings + first 4 encoder layers to reduce optimizer state\n",
        "try:\n",
        "    for n, p in model.encoder.named_parameters():\n",
        "        if n.startswith(\"embeddings.\"):\n",
        "            p.requires_grad = False\n",
        "        elif \".layer.\" in n:\n",
        "            # Handle patterns like encoder.layer.<idx>.\n",
        "            try:\n",
        "                idx = int(n.split(\".layer.\")[1].split(\".\")[0])\n",
        "                if idx < 4:\n",
        "                    p.requires_grad = False\n",
        "            except Exception:\n",
        "                pass\n",
        "        elif \".layers.\" in n:\n",
        "            # Handle patterns like encoder.layers.<idx>.\n",
        "            try:\n",
        "                idx = int(n.split(\".layers.\")[1].split(\".\")[0])\n",
        "                if idx < 4:\n",
        "                    p.requires_grad = False\n",
        "            except Exception:\n",
        "                pass\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# LLRD param groups\n",
        "if USE_LLRD:\n",
        "    n_layers = model.encoder.config.num_hidden_layers\n",
        "    base_lr, decay = LR, LLRD_DECAY\n",
        "    groups = []\n",
        "    for i in range(n_layers):\n",
        "        lr_i = base_lr * (decay ** (n_layers-1-i))\n",
        "        params_i = [p for n,p in model.named_parameters() if f\"encoder.layer.{i}.\" in n]\n",
        "        if params_i: groups.append({\"params\": params_i, \"lr\": lr_i})\n",
        "    emb_params = [p for n,p in model.named_parameters() if \"embeddings\" in n]\n",
        "    if emb_params: groups.append({\"params\": emb_params, \"lr\": base_lr * (decay ** n_layers)})\n",
        "    head_params = [p for n,p in model.named_parameters() if any(k in n for k in [\"trunk\", \"head_sent\", \"head_pol\"])]\n",
        "    if head_params: groups.append({\"params\": head_params, \"lr\": base_lr * HEAD_LR_MULT})\n",
        "    optim_params = groups\n",
        "else:\n",
        "    optim_params = model.parameters()\n",
        "\n",
        "# Decide precision flags safely (mutually exclusive)\n",
        "try:\n",
        "    BF16_FLAG = bool(torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8)\n",
        "except Exception:\n",
        "    BF16_FLAG = False\n",
        "FP16_FLAG = bool(torch.cuda.is_available() and not BF16_FLAG)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=os.path.join(OUT_DIR, \"rembert\"),\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LR,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_safetensors=False,  # avoid safetensors non-contiguous weight error\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    fp16=FP16_FLAG,\n",
        "    bf16=BF16_FLAG,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        "    report_to=\"none\",\n",
        "    optim=\"adamw_bnb_8bit\",              # 8-bit optimizer to shrink optimizer states\n",
        "    dataloader_num_workers=2,\n",
        "    dataloader_pin_memory=True,\n",
        ")\n",
        "\n",
        "trainer = MTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer),\n",
        "    task_weights=TASK_LOSS_WEIGHTS,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE)]\n",
        ")\n",
        "\n",
        "print(\"Trainingâ€¦\")\n",
        "trainer.train()\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        },
        "id": "3USnHeyIWNvz",
        "outputId": "48f028ed-f28f-47d1-b0d5-2dddfda89776"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainingâ€¦\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6557' max='11120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 6557/11120 2:09:19 < 1:30:01, 0.84 it/s, Epoch 11.78/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.555500</td>\n",
              "      <td>0.406520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.384500</td>\n",
              "      <td>0.261005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.229700</td>\n",
              "      <td>0.185196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.178700</td>\n",
              "      <td>0.117435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.106400</td>\n",
              "      <td>0.100836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.082900</td>\n",
              "      <td>0.074430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.056100</td>\n",
              "      <td>0.045224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.046100</td>\n",
              "      <td>0.037637</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11120' max='11120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11120/11120 3:46:39, Epoch 19/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.555500</td>\n",
              "      <td>0.406520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.384500</td>\n",
              "      <td>0.261005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.229700</td>\n",
              "      <td>0.185196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.178700</td>\n",
              "      <td>0.117435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.106400</td>\n",
              "      <td>0.100836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.082900</td>\n",
              "      <td>0.074430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.056100</td>\n",
              "      <td>0.045224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.046100</td>\n",
              "      <td>0.037637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.036000</td>\n",
              "      <td>0.027024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.027000</td>\n",
              "      <td>0.012389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.019000</td>\n",
              "      <td>0.009673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.015000</td>\n",
              "      <td>0.009964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.014000</td>\n",
              "      <td>0.008765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.012800</td>\n",
              "      <td>0.008768</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 7\n"
      ],
      "metadata": {
        "id": "HXvlOt6QWQSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 7: EVALUATION (TEST) â€” QUICK REPORT\n",
        "# ============================================================================\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Use the same dynamic padding collator as training to handle variable-length batches\n",
        "collate = DataCollatorWithPadding(\n",
        "    tokenizer,\n",
        "    pad_to_multiple_of=8 if (FP16_FLAG or BF16_FLAG) else None,\n",
        ")\n",
        "\n",
        "def predict(ds):\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        collate_fn=collate,\n",
        "        pin_memory=True,\n",
        "        num_workers=2,\n",
        "    )\n",
        "    logits_s, logits_p, lab_s, lab_p = [], [], [], []\n",
        "    for batch in loader:\n",
        "        for k in [\"input_ids\",\"attention_mask\",\"token_type_ids\"]:\n",
        "            if k in batch: batch[k] = batch[k].to(model.encoder.device)\n",
        "        with torch.no_grad():\n",
        "            out = model(**{k:batch[k] for k in [\"input_ids\",\"attention_mask\",\"token_type_ids\"] if k in batch})\n",
        "        logits_s.append(out[\"logits_sent\"].cpu()); logits_p.append(out[\"logits_pol\"].cpu())\n",
        "        lab_s.append(batch[\"sentiment_labels\"]); lab_p.append(batch[\"polarization_labels\"])\n",
        "    logits_s = torch.cat(logits_s); logits_p = torch.cat(logits_p)\n",
        "    y_s = torch.cat(lab_s).numpy(); y_p = torch.cat(lab_p).numpy()\n",
        "    pred_s = logits_s.argmax(-1).numpy(); pred_p = logits_p.argmax(-1).numpy()\n",
        "    return (y_s, pred_s), (y_p, pred_p)\n",
        "\n",
        "(test_s, pred_s), (test_p, pred_p) = predict(test_ds)\n",
        "print(\"=== SENTIMENT (test) ===\\n\", classification_report(test_s, pred_s, target_names=list(sent_le.classes_)))\n",
        "print(\"=== POLARIZATION (test) ===\\n\", classification_report(test_p, pred_p, target_names=list(pol_le.classes_)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32E8UVEYX160",
        "outputId": "7203795a-435f-4639-9c86-af803845271f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SENTIMENT (test) ===\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00       886\n",
            "     neutral       1.00      1.00      1.00       402\n",
            "    positive       1.00      1.00      1.00       207\n",
            "\n",
            "    accuracy                           1.00      1495\n",
            "   macro avg       1.00      1.00      1.00      1495\n",
            "weighted avg       1.00      1.00      1.00      1495\n",
            "\n",
            "=== POLARIZATION (test) ===\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "non_polarized       0.92      0.98      0.95       417\n",
            "    objective       0.93      0.92      0.93        88\n",
            "     partisan       1.00      0.97      0.98       990\n",
            "\n",
            "     accuracy                           0.97      1495\n",
            "    macro avg       0.95      0.96      0.95      1495\n",
            " weighted avg       0.97      0.97      0.97      1495\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "87YoO1iKX21o"
      }
    }
  ]
}