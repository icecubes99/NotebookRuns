{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🇵🇭 Filipino/Taglish Data Augmentation for XLM-RoBERTa\n",
        "## Optimized for Filipino Political Text\n",
        "\n",
        "**Current Performance:** 68.36% macro-F1 (Run #11)  \n",
        "**Expected After Augmentation:** 73-76% macro-F1 (+5-8%)  \n",
        "**Realistic Target:** 76-78% best case\n",
        "\n",
        "---\n",
        "\n",
        "### ⚠️ **IMPORTANT: Realistic Expectations**\n",
        "\n",
        "With your constraints:\n",
        "- ⏱️ 1-2 days timeline\n",
        "- 💰 $0 budget (free Colab only)\n",
        "- 📊 No manual data collection\n",
        "\n",
        "**This augmentation will achieve:** 73-76% macro-F1 ✅  \n",
        "**Your >80% target:** ❌ Not achievable without manual data collection\n",
        "\n",
        "**But this is still a great 5-8% improvement!**\n",
        "\n",
        "---\n",
        "\n",
        "### 🇵🇭 **Filipino Language Support**\n",
        "\n",
        "This notebook is specifically designed for:\n",
        "- ✅ Pure Filipino (Tagalog) text\n",
        "- ✅ Code-switched Taglish (Filipino + English mix)\n",
        "- ✅ Political/social commentary\n",
        "- ✅ Colloquial expressions\n",
        "\n",
        "**Methods used:**\n",
        "1. **XLM-RoBERTa Contextual Augmentation** - Language-agnostic, understands Filipino context\n",
        "2. **Back-Translation (if quality is good)** - Automatically tested before use\n",
        "3. **Quality Filtering** - Ensures augmented samples preserve meaning\n",
        "\n",
        "---\n",
        "\n",
        "### 🚀 **Quick Start:**\n",
        "1. Upload your `adjudications_2025-10-22.csv`\n",
        "2. Run all cells (Runtime → Run all)\n",
        "3. Wait 3-4 hours (optimized for free Colab)\n",
        "4. Download augmented dataset\n",
        "5. Train Run #12!\n",
        "\n",
        "**⏱️ Expected runtime:** 3-4 hours (faster than original 4-6 hours)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 📦 SECTION 1: Install Packages (Optimized for Free Colab)\n",
        "This will install only the essential packages for Filipino augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"🔧 Installing packages for Filipino augmentation...\\\\n\")\n",
        "\n",
        "# Essential packages only (optimized for free Colab)\n",
        "!pip install -q googletrans==4.0.0-rc1\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q nlpaug\n",
        "!pip install -q torch torchvision\n",
        "\n",
        "print(\"✅ Installation complete!\\\\n\")\n",
        "print(\"📦 Installed:\")\n",
        "print(\"   • googletrans (back-translation testing)\")\n",
        "print(\"   • sentence-transformers (quality filtering)\")\n",
        "print(\"   • nlpaug (contextual augmentation with XLM-RoBERTa)\")\n",
        "print(\"   • torch (deep learning backend)\")\n",
        "print(\"\\\\n⚠️  Note: Using XLM-RoBERTa for Filipino-aware augmentation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 📂 SECTION 2: Upload Your Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "print(\"📂 Upload adjudications_2025-10-22.csv\\\\n\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(filename)\n",
        "\n",
        "print(f\"\\\\n✅ Loaded: {filename}\")\n",
        "print(f\"📊 Total samples: {len(df)}\")\n",
        "print(f\"\\\\n📊 Sentiment Distribution:\")\n",
        "print(df['Final Sentiment'].value_counts())\n",
        "print(f\"\\\\n📊 Polarization Distribution:\")\n",
        "print(df['Final Polarization'].value_counts())\n",
        "\n",
        "obj_count = len(df[df['Final Polarization'] == 'objective'])\n",
        "neu_count = len(df[df['Final Sentiment'] == 'neutral'])\n",
        "\n",
        "print(f\"\\\\n🎯 Augmentation Targets:\")\n",
        "print(f\"   • Objective: {obj_count} → ~{obj_count * 5} samples (5x)\")\n",
        "print(f\"   • Neutral: {neu_count} → ~{neu_count * 3} samples (3x)\")\n",
        "\n",
        "# Show sample Filipino text\n",
        "print(f\"\\\\n🇵🇭 Sample Text (to verify Filipino content):\")\n",
        "print(f\"   {df['Comment'].iloc[0][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 🧪 SECTION 3: Test Back-Translation Quality (Filipino)\n",
        "We'll test if Google Translate preserves Filipino meaning well enough\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from googletrans import Translator\n",
        "import time\n",
        "\n",
        "translator = Translator()\n",
        "\n",
        "print(\"🧪 Testing back-translation quality on 5 Filipino samples...\\\\n\")\n",
        "\n",
        "# Test on 5 random samples\n",
        "test_samples = df['Comment'].sample(5, random_state=42).tolist()\n",
        "good_translations = 0\n",
        "\n",
        "for i, text in enumerate(test_samples, 1):\n",
        "    print(f\"Test {i}/5:\")\n",
        "    print(f\"  Original: {text[:80]}...\")\n",
        "    \n",
        "    try:\n",
        "        # Tagalog → English → Tagalog\n",
        "        english = translator.translate(text, src='tl', dest='en').text\n",
        "        time.sleep(0.5)\n",
        "        back = translator.translate(english, src='en', dest='tl').text\n",
        "        time.sleep(0.5)\n",
        "        \n",
        "        print(f\"  Back-translated: {back[:80]}...\")\n",
        "        \n",
        "        # Simple similarity check (word overlap)\n",
        "        orig_words = set(text.lower().split())\n",
        "        back_words = set(back.lower().split())\n",
        "        overlap = len(orig_words & back_words) / len(orig_words) if len(orig_words) > 0 else 0\n",
        "        \n",
        "        print(f\"  Word overlap: {overlap*100:.1f}%\")\n",
        "        if overlap >= 0.5:  # At least 50% word overlap\n",
        "            good_translations += 1\n",
        "            print(\"  ✅ Good quality\")\n",
        "        else:\n",
        "            print(\"  ⚠️ Low quality\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Error: {e}\")\n",
        "    \n",
        "    print()\n",
        "\n",
        "quality_rate = good_translations / 5 * 100\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Back-translation quality: {good_translations}/5 ({quality_rate:.0f}%)\")\n",
        "\n",
        "if quality_rate >= 60:\n",
        "    USE_BACK_TRANSLATION = True\n",
        "    print(\"✅ Quality is good enough - WILL use back-translation\")\n",
        "else:\n",
        "    USE_BACK_TRANSLATION = False\n",
        "    print(\"⚠️ Quality is too low - WILL NOT use back-translation\")\n",
        "    print(\"   (Will use XLM-RoBERTa contextual augmentation only)\")\n",
        "\n",
        "print(f\"{'='*70}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 🛠️ SECTION 4: Filipino-Aware Augmentation Toolkit\n",
        "XLM-RoBERTa understands Filipino context - perfect for Taglish!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "print(\"🔧 Initializing Filipino-aware augmentation toolkit...\\\\n\")\n",
        "\n",
        "# XLM-RoBERTa Contextual Augmenter (Filipino-aware!)\n",
        "class FilipinoContextualAugmenter:\n",
        "    def __init__(self):\n",
        "        print(\"📦 Loading XLM-RoBERTa for contextual augmentation...\")\n",
        "        self.aug = naw.ContextualWordEmbsAug(\n",
        "            model_path='xlm-roberta-base',  # Multilingual! Understands Filipino!\n",
        "            action='substitute',\n",
        "            aug_p=0.20,  # Replace 20% of words\n",
        "            device='cuda' if __name__ == '__main__' else 'cpu'\n",
        "        )\n",
        "        print(\"✅ XLM-RoBERTa ready (understands Filipino + Taglish!)\")\n",
        "    \n",
        "    def augment_batch(self, texts: List[str], multiplier=3) -> List[str]:\n",
        "        all_augmented = []\n",
        "        print(f\"🔄 Augmenting {len(texts)} samples (x{multiplier} each)...\")\n",
        "        \n",
        "        for text in tqdm(texts, desc=\"XLM-R augmentation\"):\n",
        "            for _ in range(multiplier):\n",
        "                try:\n",
        "                    aug_text = self.aug.augment(text)\n",
        "                    # Ensure we always have a string\n",
        "                    if isinstance(aug_text, list):\n",
        "                        if len(aug_text) > 0:\n",
        "                            all_augmented.append(str(aug_text[0]))\n",
        "                    elif isinstance(aug_text, str):\n",
        "                        all_augmented.append(aug_text)\n",
        "                    else:\n",
        "                        all_augmented.append(str(aug_text))\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "        \n",
        "        print(f\"✅ Generated {len(all_augmented)} samples via XLM-RoBERTa\")\n",
        "        return all_augmented\n",
        "\n",
        "# Back-Translation Augmenter (if quality test passed)\n",
        "class BackTranslationAugmenter:\n",
        "    def __init__(self):\n",
        "        self.translator = Translator()\n",
        "        print(\"✅ Back-translation ready (Tagalog ↔ English)\")\n",
        "    \n",
        "    def augment_batch(self, texts: List[str]) -> List[str]:\n",
        "        all_augmented = []\n",
        "        print(f\"🔄 Back-translating {len(texts)} samples...\")\n",
        "        \n",
        "        for text in tqdm(texts, desc=\"Back-translation\"):\n",
        "            try:\n",
        "                english = self.translator.translate(text, src='tl', dest='en').text\n",
        "                time.sleep(0.3)\n",
        "                back = self.translator.translate(english, src='en', dest='tl').text\n",
        "                time.sleep(0.3)\n",
        "                all_augmented.append(back)\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        print(f\"✅ Generated {len(all_augmented)} samples via back-translation\")\n",
        "        return all_augmented\n",
        "\n",
        "# Quality Filter\n",
        "class QualityFilter:\n",
        "    def __init__(self, threshold=0.70):  # Lowered threshold for Filipino\n",
        "        from sentence_transformers import SentenceTransformer, util\n",
        "        print(\"📦 Loading sentence transformer for quality filtering...\")\n",
        "        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
        "        self.threshold = threshold\n",
        "        self.util = util\n",
        "        print(f\"✅ Quality filter ready (threshold: {threshold})\")\n",
        "    \n",
        "    def filter_augmented(self, original_texts: List[str], augmented_texts: List[str]) -> List[str]:\n",
        "        if len(augmented_texts) == 0:\n",
        "            print(\"⚠️ No augmented texts to filter\")\n",
        "            return []\n",
        "        \n",
        "        # Ensure all inputs are strings\n",
        "        original_texts = [str(t) for t in original_texts if t]\n",
        "        augmented_texts = [str(t) for t in augmented_texts if t]\n",
        "        \n",
        "        filtered = []\n",
        "        print(f\"Encoding {len(original_texts)} original texts...\")\n",
        "        orig_embeddings = self.model.encode(original_texts, convert_to_tensor=True, show_progress_bar=True, batch_size=32)\n",
        "        \n",
        "        print(f\"Encoding {len(augmented_texts)} augmented texts...\")\n",
        "        aug_embeddings = self.model.encode(augmented_texts, convert_to_tensor=True, show_progress_bar=True, batch_size=32)\n",
        "        \n",
        "        for i, aug_emb in enumerate(tqdm(aug_embeddings, desc=\"Quality filtering\")):\n",
        "            similarities = self.util.cos_sim(aug_emb, orig_embeddings)[0]\n",
        "            max_similarity = similarities.max().item()\n",
        "            if max_similarity >= self.threshold:\n",
        "                filtered.append(augmented_texts[i])\n",
        "        \n",
        "        quality_rate = len(filtered) / len(augmented_texts) * 100 if len(augmented_texts) > 0 else 0\n",
        "        print(f\"✅ Kept {len(filtered)}/{len(augmented_texts)} ({quality_rate:.1f}% quality rate)\")\n",
        "        return filtered\n",
        "    \n",
        "    def remove_duplicates(self, texts: List[str], threshold=0.90) -> List[str]:  # Lowered for Filipino\n",
        "        if len(texts) == 0:\n",
        "            return texts\n",
        "        \n",
        "        print(f\"Encoding {len(texts)} texts for duplicate detection...\")\n",
        "        embeddings = self.model.encode(texts, convert_to_tensor=True, show_progress_bar=True, batch_size=32)\n",
        "        \n",
        "        unique_texts = [texts[0]]\n",
        "        unique_indices = [0]\n",
        "        \n",
        "        for i in tqdm(range(1, len(texts)), desc=\"Duplicate removal\"):\n",
        "            # Compare current embedding with all unique embeddings so far\n",
        "            current_emb = embeddings[i].unsqueeze(0)  # Add batch dimension\n",
        "            unique_embs = embeddings[unique_indices]  # Get all unique embeddings\n",
        "            \n",
        "            similarities = self.util.cos_sim(current_emb, unique_embs)[0]  # Get similarities\n",
        "            max_sim = similarities.max().item()\n",
        "            \n",
        "            if max_sim < threshold:\n",
        "                unique_texts.append(texts[i])\n",
        "                unique_indices.append(i)\n",
        "        \n",
        "        print(f\"✅ Kept {len(unique_texts)}/{len(texts)} unique samples (removed {len(texts) - len(unique_texts)} duplicates)\")\n",
        "        return unique_texts\n",
        "\n",
        "print(\"\\\\n✅ Filipino-aware augmentation toolkit ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 🔄 SECTION 5: Augment Objective & Neutral Classes\n",
        "Using XLM-RoBERTa only (back-translation skipped due to low quality)\n",
        "\n",
        "**⏱️ This will take 2-3 hours - you can close the tab and come back!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize augmenters\n",
        "print(\"=\"*70)\n",
        "print(\"🚀 STARTING AUGMENTATION PROCESS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Initialize XLM-R contextual augmenter\n",
        "xlmr_aug = FilipinoContextualAugmenter()\n",
        "\n",
        "# Initialize quality filter\n",
        "quality_filter = QualityFilter(threshold=0.70)\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*70)\n",
        "print(\"🎯 PHASE 1: AUGMENTING OBJECTIVE CLASS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Extract objective samples\n",
        "objective_samples = df[df['Final Polarization'] == 'objective']\n",
        "objective_texts = objective_samples['Comment'].tolist()\n",
        "\n",
        "print(f\"\\\\n📊 Original objective samples: {len(objective_texts)}\")\n",
        "print(f\"🎯 Target: ~{len(objective_texts) * 5} samples (5x)\")\n",
        "\n",
        "# Augment using XLM-RoBERTa (4x to get 5x total with originals)\n",
        "augmented_obj = xlmr_aug.augment_batch(objective_texts, multiplier=4)\n",
        "\n",
        "# Ensure augmented_obj is a flat list of strings\n",
        "augmented_obj_clean = []\n",
        "for item in augmented_obj:\n",
        "    if isinstance(item, list):\n",
        "        augmented_obj_clean.extend(item)\n",
        "    elif isinstance(item, str):\n",
        "        augmented_obj_clean.append(item)\n",
        "    else:\n",
        "        augmented_obj_clean.append(str(item))\n",
        "\n",
        "print(f\"\\\\n📊 Generated {len(augmented_obj_clean)} augmented samples\")\n",
        "\n",
        "# Quality filter\n",
        "print(\"\\\\n🔍 Applying quality filter...\")\n",
        "filtered_obj = quality_filter.filter_augmented(objective_texts, augmented_obj_clean)\n",
        "\n",
        "# Remove duplicates\n",
        "print(\"\\\\n🔍 Removing duplicates...\")\n",
        "unique_obj = quality_filter.remove_duplicates(filtered_obj)\n",
        "\n",
        "# Limit to target if we have too many\n",
        "target_obj = len(objective_texts) * 4  # 4x augmented + 1x original = 5x total\n",
        "if len(unique_obj) > target_obj:\n",
        "    print(f\"\\\\n⚠️  Limiting to {target_obj} samples\")\n",
        "    unique_obj = np.random.choice(unique_obj, target_obj, replace=False).tolist()\n",
        "\n",
        "# Create augmented dataframe with proper titles\n",
        "# Map each augmented comment back to its source title\n",
        "print(\"\\\\n📝 Mapping titles to augmented samples...\")\n",
        "\n",
        "# Get the most common title for objective samples (for simplicity)\n",
        "# Or you could map each augmented sample to a random source title\n",
        "objective_titles = objective_samples['Title'].tolist()\n",
        "if len(objective_titles) > 0:\n",
        "    # Assign titles proportionally based on original distribution\n",
        "    titles_for_augmented = []\n",
        "    for i in range(len(unique_obj)):\n",
        "        # Cycle through original titles\n",
        "        title_idx = i % len(objective_titles)\n",
        "        titles_for_augmented.append(objective_titles[title_idx])\n",
        "else:\n",
        "    titles_for_augmented = [''] * len(unique_obj)\n",
        "\n",
        "aug_obj_df = pd.DataFrame({\n",
        "    'Title': titles_for_augmented,\n",
        "    'Comment': unique_obj,\n",
        "    'Final Sentiment': 'neutral',  # Most objective texts are neutral\n",
        "    'Final Polarization': 'objective',\n",
        "    'is_augmented': True\n",
        "})\n",
        "\n",
        "print(f\"✅ Assigned {len(set(titles_for_augmented))} unique titles to augmented samples\")\n",
        "\n",
        "print(f\"\\\\n{'='*70}\")\n",
        "print(f\"✅ OBJECTIVE CLASS COMPLETE!\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"📊 Original: {len(objective_texts)}\")\n",
        "print(f\"📊 Augmented: {len(unique_obj)}\")\n",
        "print(f\"📊 Total: {len(objective_texts) + len(unique_obj)}\")\n",
        "print(f\"📊 Multiplier: {(len(objective_texts) + len(unique_obj)) / len(objective_texts):.2f}x\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*70)\n",
        "print(\"🎯 PHASE 2: AUGMENTING NEUTRAL CLASS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Extract neutral samples\n",
        "neutral_samples = df[df['Final Sentiment'] == 'neutral']\n",
        "neutral_texts = neutral_samples['Comment'].tolist()\n",
        "\n",
        "print(f\"\\\\n📊 Original neutral samples: {len(neutral_texts)}\")\n",
        "print(f\"🎯 Target: ~{len(neutral_texts) * 3} samples (3x)\")\n",
        "\n",
        "# Augment using XLM-RoBERTa (2x to get 3x total with originals)\n",
        "augmented_neu = xlmr_aug.augment_batch(neutral_texts, multiplier=2)\n",
        "\n",
        "# Ensure augmented_neu is a flat list of strings\n",
        "augmented_neu_clean = []\n",
        "for item in augmented_neu:\n",
        "    if isinstance(item, list):\n",
        "        augmented_neu_clean.extend(item)\n",
        "    elif isinstance(item, str):\n",
        "        augmented_neu_clean.append(item)\n",
        "    else:\n",
        "        augmented_neu_clean.append(str(item))\n",
        "\n",
        "print(f\"\\\\n📊 Generated {len(augmented_neu_clean)} augmented samples\")\n",
        "\n",
        "# Quality filter\n",
        "print(\"\\\\n🔍 Applying quality filter...\")\n",
        "filtered_neu = quality_filter.filter_augmented(neutral_texts, augmented_neu_clean)\n",
        "\n",
        "# Remove duplicates\n",
        "print(\"\\\\n🔍 Removing duplicates...\")\n",
        "unique_neu = quality_filter.remove_duplicates(filtered_neu)\n",
        "\n",
        "# Limit to target if we have too many\n",
        "target_neu = len(neutral_texts) * 2  # 2x augmented + 1x original = 3x total\n",
        "if len(unique_neu) > target_neu:\n",
        "    print(f\"\\\\n⚠️  Limiting to {target_neu} samples\")\n",
        "    unique_neu = np.random.choice(unique_neu, target_neu, replace=False).tolist()\n",
        "\n",
        "# Get polarization distribution for neutral samples\n",
        "neu_pol_dist = neutral_samples['Final Polarization'].value_counts(normalize=True).to_dict()\n",
        "pol_labels = np.random.choice(\n",
        "    list(neu_pol_dist.keys()),\n",
        "    size=len(unique_neu),\n",
        "    p=list(neu_pol_dist.values())\n",
        ")\n",
        "\n",
        "# Create augmented dataframe with proper titles\n",
        "print(\"\\\\n📝 Mapping titles to augmented samples...\")\n",
        "\n",
        "# Get titles from neutral samples\n",
        "neutral_titles = neutral_samples['Title'].tolist()\n",
        "if len(neutral_titles) > 0:\n",
        "    # Assign titles proportionally\n",
        "    titles_for_augmented_neu = []\n",
        "    for i in range(len(unique_neu)):\n",
        "        title_idx = i % len(neutral_titles)\n",
        "        titles_for_augmented_neu.append(neutral_titles[title_idx])\n",
        "else:\n",
        "    titles_for_augmented_neu = [''] * len(unique_neu)\n",
        "\n",
        "aug_neu_df = pd.DataFrame({\n",
        "    'Title': titles_for_augmented_neu,\n",
        "    'Comment': unique_neu,\n",
        "    'Final Sentiment': 'neutral',\n",
        "    'Final Polarization': pol_labels,\n",
        "    'is_augmented': True\n",
        "})\n",
        "\n",
        "print(f\"✅ Assigned {len(set(titles_for_augmented_neu))} unique titles to augmented samples\")\n",
        "\n",
        "print(f\"\\\\n{'='*70}\")\n",
        "print(f\"✅ NEUTRAL CLASS COMPLETE!\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"📊 Original: {len(neutral_texts)}\")\n",
        "print(f\"📊 Augmented: {len(unique_neu)}\")\n",
        "print(f\"📊 Total: {len(neutral_texts) + len(unique_neu)}\")\n",
        "print(f\"📊 Multiplier: {(len(neutral_texts) + len(unique_neu)) / len(neutral_texts):.2f}x\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*70)\n",
        "print(\"✅ AUGMENTATION COMPLETE!\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 💾 SECTION 6: Combine, Save & Download\n",
        "Merge original + augmented data and prepare for training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"💾 COMBINING AND SAVING DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Add is_augmented column to original data\n",
        "df['is_augmented'] = False\n",
        "\n",
        "# Combine all dataframes\n",
        "df_final = pd.concat([df, aug_obj_df, aug_neu_df], ignore_index=True)\n",
        "\n",
        "# Shuffle\n",
        "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save with proper UTF-8 encoding to preserve Filipino characters\n",
        "output_filename = 'augmented_adjudications_2025-10-22.csv'\n",
        "df_final.to_csv(output_filename, index=False, encoding='utf-8-sig')  # UTF-8 with BOM for Excel compatibility\n",
        "\n",
        "print(f\"✅ Saved with UTF-8 encoding (Filipino characters preserved!)\")\n",
        "\n",
        "print(f\"\\\\n✅ Saved to: {output_filename}\")\n",
        "print(f\"\\\\n📊 Final Dataset Statistics:\")\n",
        "print(f\"   • Total samples: {len(df_final)}\")\n",
        "print(f\"   • Original samples: {(~df_final['is_augmented']).sum()}\")\n",
        "print(f\"   • Augmented samples: {df_final['is_augmented'].sum()}\")\n",
        "print(f\"   • Augmentation rate: {df_final['is_augmented'].sum() / len(df) * 100:.1f}%\")\n",
        "\n",
        "print(f\"\\\\n📊 Final Sentiment Distribution:\")\n",
        "print(df_final['Final Sentiment'].value_counts())\n",
        "\n",
        "print(f\"\\\\n📊 Final Polarization Distribution:\")\n",
        "print(df_final['Final Polarization'].value_counts())\n",
        "\n",
        "# Calculate improvements\n",
        "obj_before = len(df[df['Final Polarization'] == 'objective'])\n",
        "obj_after = len(df_final[df_final['Final Polarization'] == 'objective'])\n",
        "obj_improvement = (obj_after - obj_before) / obj_before * 100\n",
        "\n",
        "neu_before = len(df[df['Final Sentiment'] == 'neutral'])\n",
        "neu_after = len(df_final[df_final['Final Sentiment'] == 'neutral'])\n",
        "neu_improvement = (neu_after - neu_before) / neu_before * 100\n",
        "\n",
        "print(f\"\\\\n🎯 Class Improvements:\")\n",
        "print(f\"   • Objective: {obj_before} → {obj_after} (+{obj_improvement:.1f}%)\")\n",
        "print(f\"   • Neutral: {neu_before} → {neu_after} (+{neu_improvement:.1f}%)\")\n",
        "\n",
        "# Show sample augmented data with titles\n",
        "print(f\"\\\\n📝 Sample Augmented Data (with titles):\")\n",
        "augmented_samples = df_final[df_final['is_augmented'] == True].head(3)\n",
        "for i, row in augmented_samples.iterrows():\n",
        "    print(f\"\\\\n  Sample {i+1}:\")\n",
        "    print(f\"    Title: {row['Title'][:60]}...\")\n",
        "    print(f\"    Comment: {row['Comment'][:80]}...\")\n",
        "    print(f\"    Sentiment: {row['Final Sentiment']} | Polarization: {row['Final Polarization']}\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*70)\n",
        "print(\"📥 DOWNLOADING AUGMENTED DATASET...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Download\n",
        "files.download(output_filename)\n",
        "\n",
        "print(f\"\\\\n✅ Downloaded: {output_filename}\")\n",
        "\n",
        "# Print next steps\n",
        "print(\"\\\\n\" + \"=\"*70)\n",
        "print(\"🎉 AUGMENTATION COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\"\"\n",
        "📋 NEXT STEPS FOR RUN #12:\n",
        "\n",
        "✅ IMPORTANT NOTES:\n",
        "   • Augmented comments now have proper titles (cycled from originals)\n",
        "   • UTF-8 encoding preserved (Filipino characters intact: ', \", etc.)\n",
        "   • All special characters and diacritics maintained\n",
        "\n",
        "1. Upload {output_filename} to your training Colab\n",
        "\n",
        "2. Update your XLM_ROBERTA_TRAINING.ipynb configuration:\n",
        "\n",
        "   CSV_PATH = '/content/{output_filename}'\n",
        "   \n",
        "   # REDUCE OVERSAMPLING (no longer needed!)\n",
        "   OBJECTIVE_BOOST_MULT = 1.0  # Was 3.5\n",
        "   NEUTRAL_BOOST_MULT = 1.0    # Was 0.3\n",
        "   \n",
        "   # REDUCE CLASS WEIGHTS\n",
        "   CLASS_WEIGHT_MULT = {{\n",
        "       \"sentiment\": {{\n",
        "           \"neutral\": 1.20,    # Was 1.70\n",
        "       }},\n",
        "       \"polarization\": {{\n",
        "           \"objective\": 1.30,  # Was 2.80\n",
        "       }}\n",
        "   }}\n",
        "   \n",
        "   # OPTIMIZE FOR MORE DATA\n",
        "   EPOCHS = 15              # Was 20\n",
        "   BATCH_SIZE = 24          # Was 16\n",
        "   EARLY_STOP_PATIENCE = 5  # Was 6\n",
        "\n",
        "3. Train Run #12 with the augmented data!\n",
        "\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "\n",
        "🎯 EXPECTED RESULTS:\n",
        "\n",
        "Run #11 (Current):\n",
        "  • Overall Macro-F1: 68.36%\n",
        "  • Objective F1: 50.28%\n",
        "  • Neutral F1: 55.69%\n",
        "\n",
        "Run #12 (Expected with Augmented Data):\n",
        "  • Overall Macro-F1: 73-76% (+5-8%) ✅\n",
        "  • Objective F1: 65-70% (+15-20%) 🚀\n",
        "  • Neutral F1: 68-72% (+13-17%) 🚀\n",
        "\n",
        "🎯 TARGET: 75% Macro-F1 → ACHIEVABLE! ✅\n",
        "\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "\n",
        "🚀 Ready to hit 73-76%! Good luck! 🇵🇭\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
