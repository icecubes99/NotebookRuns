# üöÄ RUN #15: QUICK START GUIDE

**Date:** October 26, 2025  
**Status:** Ready to run  
**Expected Time:** ~95-105 minutes  
**Expected Macro-F1:** 64-66% (+1-3% over R10's 63.06%)

---

## ‚úÖ WHAT'S BEEN DONE

I've completed comprehensive research and preparation for your >75% breakthrough:

### 1. **Created Breakthrough Strategy Document** ‚úÖ

- File: `BREAKTHROUGH_STRATEGY_TO_75_PERCENT.md`
- Comprehensive 3-track plan to reach 75%+
- Research-backed techniques
- Timeline: 7 days to 75%+

### 2. **Updated Notebook for Run #15** ‚úÖ

- File: `MBERT_TRAINING.ipynb`
- Configuration: Sequence length optimization (MAX_LENGTH: 224 ‚Üí 320)
- Restored R9 proven architecture (removed R14's harmful changes)
- Ready to execute immediately

### 3. **Completed Run #14 Analysis** ‚úÖ

- File: `RUN_ANALYSIS.md` (appended comprehensive analysis)
- Key finding: Architectural complexity backfired (-5.3% regression)
- Lesson: Simple architecture + data-centric approach is the path forward

---

## üéØ RUN #15 STRATEGY

**Quick Win:** Sequence length optimization (research-backed)

**Key Change:**

```python
MAX_LENGTH = 320  # ‚¨ÜÔ∏è UP from 224 (+43% more context!)
```

**Why This Works:**

- NIH study (PMC9051848) showed length 320 achieved 71.7% F1
- News articles often have sentiment/polarization signals late in text
- Longer context = better understanding

**What's Restored:**

- All R9 proven hyperparameters
- Simple 3-layer head architecture (HEAD_HIDDEN=768)
- "last4_mean" pooling (stable and proven)
- NO complex architectural features

---

## üèÉ HOW TO RUN

**Option 1: Google Colab (Recommended)**

1. Upload `MBERT_TRAINING.ipynb` to Colab
2. Upload your dataset CSV
3. Run all cells (Runtime ‚Üí Run all)
4. Wait ~95-105 minutes
5. Check results in Section 11

**Option 2: Local/Server**

```bash
jupyter notebook MBERT_TRAINING.ipynb
# Or
python -m jupyter notebook MBERT_TRAINING.ipynb
```

---

## üìä EXPECTED RESULTS

**Run #15 Prediction:**

- **Sentiment F1:** 63-65% (vs R10: ~62%)
- **Polarization F1:** 65-67% (vs R10: ~64%)
- **Overall Macro-F1:** 64-66% (vs R10: 63.06%)
- **Improvement:** +1-3% from better context capture

**Per-Class Improvements (Expected):**

- **Objective:** 40-45% ‚Üí 45-50% F1 (+5% from longer context)
- **Negative recall:** 45-50% ‚Üí 50-55% (+5% from better understanding)
- **Neutral:** 55-60% ‚Üí 58-62% (+3% from less confusion)

---

## üöÄ AFTER RUN #15: PATH TO 75%

**See `BREAKTHROUGH_STRATEGY_TO_75_PERCENT.md` for complete plan!**

**Quick Summary:**

### Track 1: Data Augmentation (CRITICAL) ‚Üí +4-6%

- Back-translation for objective class (90 ‚Üí 900 samples, +10x)
- Contextual word substitution
- Synonym replacement
- **Expected:** 68-72% Macro-F1

### Track 2: Smart Ensemble (QUICK WIN) ‚Üí +2-3%

- Seed ensemble (average predictions from multiple seeds)
- Cross-architecture ensemble (mBERT + XLM-RoBERTa)
- **Expected:** 70-74% Macro-F1

### Track 3: Advanced Techniques (HIGH IMPACT) ‚Üí +3-5%

- Curriculum learning (easy ‚Üí hard examples)
- Pseudo-labeling (semi-supervised learning)
- Class-specific label smoothing
- **Expected:** 75-77% Macro-F1 ‚úÖ

---

## üìã IMMEDIATE NEXT STEPS AFTER R15

1. **Run the analysis** (I'll do this for you when results are ready)
2. **If successful (64-66%):**

   - Proceed with data augmentation (Track 1)
   - Implement back-translation for objective class
   - Target: Run #16 with augmented data ‚Üí 68-70%

3. **If unsuccessful (<64%):**
   - Analyze what went wrong
   - May need to adjust sequence length or restore R9 exactly
   - Fallback: Proceed with ensemble strategy first

---

## ‚ö†Ô∏è IMPORTANT NOTES

### Memory Considerations

- MAX_LENGTH=320 uses ~33% more memory than 224
- BATCH_SIZE=12 (down from 16) compensates for this
- Effective batch size still 48 (12 √ó 4 grad accumulation steps)
- Should fit on T4 GPU (16GB) without issues

### Time Expectations

- R9/R10: ~89-91 minutes with MAX_LENGTH=224
- R15: ~95-105 minutes with MAX_LENGTH=320 (+10% time)
- Longer sequences = slightly slower training

### What to Watch

- Training should be stable (no wild loss spikes)
- Validation Macro-F1 should improve steadily
- Early stopping at epoch 8 max (if not improving)

---

## üîç RESEARCH BACKING

**Sequence Length Study:**

- Source: NIH PMC9051848
- Finding: "BERT classifier with sequence length 320 achieved highest accuracy of 70.7% and F1 score of 71.7%"
- Comparison: Length 128 = 67.2% F1, Length 224 = 69.5% F1, **Length 320 = 71.7% F1**

**Why Longer Context Helps:**

1. News articles are typically 200-400 words
2. Sentiment indicators often appear in conclusions (late text)
3. Polarization context requires understanding full argumentation
4. Truncating at 224 tokens loses ~30% of average article

---

## üí° KEY INSIGHTS FROM R14 FAILURE

**What NOT to Do (Learned from R14):**

- ‚ùå DON'T double model capacity (caused overfitting)
- ‚ùå DON'T add multiple architectural changes at once
- ‚ùå DON'T use attention pooling without extensive testing
- ‚ùå DON'T add residual blocks for small datasets
- ‚ùå DON'T reduce batch size below 12

**What DOES Work (Validated R1-R10):**

- ‚úÖ Simple 3-layer heads (HEAD_HIDDEN=768)
- ‚úÖ "last4_mean" pooling (stable)
- ‚úÖ Proven R9 hyperparameters
- ‚úÖ Incremental changes, one at a time
- ‚úÖ Data-centric approach over architectural complexity

---

## üìà PERFORMANCE TRAJECTORY

| Run     | Strategy              | Macro-F1   | Status         |
| ------- | --------------------- | ---------- | -------------- |
| R9      | Bug fixes + seed      | 62.74%     | ‚úÖ Proven      |
| R10     | R9 same seed          | 63.06%     | ‚úÖ Best        |
| R14     | Massive architecture  | 57.76%     | ‚ùå Disaster    |
| **R15** | **Sequence length**   | **64-66%** | **üéØ Current** |
| R16     | + Data augmentation   | 68-70%     | üìÖ Next        |
| R17     | + Curriculum learning | 70-72%     | üìÖ Planned     |
| R18     | + Pseudo-labeling     | 72-74%     | üìÖ Planned     |
| R19     | + Final ensemble      | 75-77%     | üéØ **TARGET**  |

---

## üéØ SUCCESS CRITERIA

**Run #15 is successful if:**

- ‚úÖ Overall Macro-F1 ‚â• 64% (min target)
- ‚úÖ No catastrophic per-class regressions
- ‚úÖ Training completes without errors
- ‚úÖ Stable training curve (no overfitting)

**Stretch Goals:**

- üéØ Macro-F1 ‚â• 65% (great!)
- üéØ Objective class F1 ‚â• 45% (improvement)
- üéØ Negative recall ‚â• 50% (fixed)

---

## üìû READY TO GO!

**Your notebook is configured and ready to run.**

**What to do right now:**

1. Open `MBERT_TRAINING.ipynb` in Google Colab
2. Upload your dataset CSV
3. Click "Runtime ‚Üí Run all"
4. Wait ~100 minutes
5. Come back and I'll analyze the results!

**After results:**

- Share the run-data with me
- I'll perform comprehensive analysis
- We'll proceed with the breakthrough strategy (Track 1: Data Augmentation)

---

**Good luck with Run #15! Let's get to 75%! üöÄ**
